{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec9298c3-efe9-40a3-a485-83250611688f",
   "metadata": {},
   "source": [
    "# TODO \n",
    "\n",
    "* Сделай аттеншн"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6092b42-6423-4b4f-81b3-b6ddec64ecf1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c94665-4148-40ed-b2a9-516dd3becc7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "import tqdm.notebook as tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c231a4b1-bebe-47eb-b428-729791170259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bob1ch/Рабочий стол/NN-NLP/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#import keras\n",
    "import os\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import sklearn.metrics\n",
    "#import tensorflow as tf\n",
    "import tqdm.notebook as tqdm\n",
    "import sklearn.model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#pytorch\n",
    "\n",
    "import torch\n",
    "from torcheval.metrics import BLEUScore\n",
    "#from torchmetrics.classification import MultilabelAUROC\n",
    "#from torchsummary import summary\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9490154-b4d9-4fe5-b596-16b3042d24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'models'\n",
    "MAX_TOKENS = 32\n",
    "MAX_LENGTH = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c18c6e-1c88-4e6b-8f13-476b78b88a2b",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e38ac2-a5e2-4435-8c95-0982e3039768",
   "metadata": {},
   "source": [
    "Get the dataset from [here](https://tatoeba.org/en/downloads). Preferably use russian to english translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e09a5d-8537-4ffa-8583-f09c30028b2d",
   "metadata": {},
   "source": [
    "Use a custom tokenizer that can add bos and eos tokens (pass `add_special_tokens=True` when calling the tokenizer to add them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8312a719-f1cd-4982-a2e8-2178a7abe850",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Tokenizer(transformers.GPT2Tokenizer):\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        if token_ids_1 is None:\n",
    "            return [self.bos_token_id, *token_ids_0, self.eos_token_id]\n",
    "\n",
    "        return [self.bos_token_id, *token_ids_0, self.bos_token_id, *token_ids_1, self.eos_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b187545e-882a-4cd1-8bab-dcec2f33b5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'Tokenizer'.\n",
      "/home/bob1ch/Рабочий стол/NN-NLP/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afff2909-320a-40b0-81bb-7990b09bc319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tokenize(data):\n",
    "    return tokenizer(data, max_length=MAX_TOKENS, truncation=True, padding=True)['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f4f399-308a-457b-a3ce-a8ede414603e",
   "metadata": {},
   "source": [
    "Since the dataset is rather large, you can omit the validation dataset and just use a set of test sentences after the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff78613-15ce-4695-aced-4379ec067765",
   "metadata": {},
   "source": [
    "Create a dataset that returns the following\n",
    "* A pair of tensors `((None, L), (None, P))` -- input sequence of tokens and output sequence of tokens to be fed into decoder (this should start with the BOS token)\n",
    "* A tensor `(None, P)` -- output sequence of tokens to be predicted (this should end with EOS token)\n",
    "* A tensor `(None, P)` -- a masking tensor marking padded tokens with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1abf130f-0d7f-4df8-bb8d-a772685bd584",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('seq2seq_dataset.tsv', \n",
    "            sep='\\t', \n",
    "            on_bad_lines='skip',\n",
    "            names=['id_1', 'rus', 'id_2', 'eng'])[['rus', 'eng']]#тут скип, потомучта какое-то говно возникало\n",
    "\n",
    "X, y = to_tokenize(data['rus'].to_list()), to_tokenize(data['eng'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f05fd112-eb16-4d0c-a1d9-217e8bfb7e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size = 0.7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "834aa559-0808-493c-ae5c-86f3dcefaa18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50256,   141,   220, ...,   252,   140, 50256],\n",
       "       [50256, 35072,   109, ...,   242,   140, 50256],\n",
       "       [50256, 50256, 50256, ...,   250,   140, 50256],\n",
       "       ...,\n",
       "       [50256, 50256, 50256, ...,   250,   140, 50256],\n",
       "       [50256, 50256, 50256, ...,   250,   140, 50256],\n",
       "       [50256, 50256, 50256, ...,   252,   140, 50256]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X)[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf228cef-62a1-40e8-9c34-a685410d0752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "    (inp, out_BOS, out_EOS, masking_PAD) = zip(*batch)\n",
    "    inp = torch.nn.utils.rnn.pad_sequence(inp, batch_first=True, padding_value=tokenizer.eos_token_id)\n",
    "    out_BOS = torch.nn.utils.rnn.pad_sequence(out_BOS, batch_first=True, padding_value=tokenizer.eos_token_id)\n",
    "    out_EOS = torch.nn.utils.rnn.pad_sequence(out_EOS, batch_first=True, padding_value=tokenizer.eos_token_id)\n",
    "    masking_PAD = torch.nn.utils.rnn.pad_sequence(masking_PAD, batch_first=True, padding_value=tokenizer.eos_token_id)\n",
    "\n",
    "    return inp, out_BOS, out_EOS, masking_PAD\n",
    "\n",
    "def get_dataloader(batch, X, y):\n",
    "    X = np.array(X)[:, ::-1] #Торч поругался, что я не могу сделать отрицательный степ\n",
    "    inp = torch.LongTensor(X.copy()).to('cuda') #а потом он ещё поругался, что в массиве у меня отрицательный страйд и попросил .copy()\n",
    "    out_BOS = torch.LongTensor(y).to('cuda')[:, :-1]\n",
    "    out_EOS = torch.LongTensor(y).to('cuda')[:, 1:]\n",
    "    masking_PAD = (out_EOS != tokenizer.pad_token_id).to('cuda')\n",
    "    #masking_PAD[:, 0] =  1 #BOS==EOS поэтому тут ручками доделал\n",
    "    data = torch.utils.data.TensorDataset(inp,\n",
    "                                          out_BOS,\n",
    "                                          out_EOS,\n",
    "                                          masking_PAD)\n",
    "    return torch.utils.data.DataLoader(data, batch_size=batch, shuffle=True, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfe5eee3-003e-4192-b8ec-5e033d0ff8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_dataloader(64, X_train, y_train)\n",
    "data_test = get_dataloader(64, X_test[:20], y_test[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff24dcac-c9f8-4d56-b584-5208c423f196",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c219a81a-59ea-4b56-96da-caa0e24c3db5",
   "metadata": {},
   "source": [
    "Create a model for training. The model should have two inputs: input sequence `(None, L)` and output sequence`(None, P)`. The model output is a single tensor `(None, P)` logits (or probabilities) of the next token predicted for each input one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "03a5f240-ffa9-4e13-b73a-575a0b76979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, units: int, n_tokens: int, n_stacks: int, bidirectional: bool, name: str):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.name = name\n",
    "        self.units = units\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.embeddings = torch.nn.Embedding(n_tokens, units)\n",
    "        self.rnn = torch.nn.GRU(units, units, num_layers=n_stacks, bidirectional=bidirectional, batch_first=True) #можно поменять\n",
    "\n",
    "    def forward(self, text):\n",
    "        embeds = self.embeddings(text)      \n",
    "        out, h = self.rnn(embeds)\n",
    "\n",
    "        return out, h\n",
    "\n",
    "# class BahdanauAttention(torch.nn.Module):\n",
    "#     def __init__(self, hidden_size):\n",
    "#         super(BahdanauAttention, self).__init__()\n",
    "#         self.Wa = torch.nn.Linear(hidden_size, hidden_size)\n",
    "#         self.Ua = torch.nn.Linear(hidden_size, hidden_size)\n",
    "#         self.Va = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "#     def forward(self, query, keys):\n",
    "#         scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "#         scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "#         weights = F.softmax(scores, dim=-1)\n",
    "#         context = torch.bmm(weights, keys)\n",
    "\n",
    "#         return context, weights\n",
    "\n",
    "#https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#the-decoder\n",
    "class Decoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, units: int, n_tokens: int, n_stacks: int, bidirectional: bool, name: str): \n",
    "        super(Decoder, self).__init__()\n",
    "        self.name = name\n",
    "        self.units = units\n",
    "        #self.bidirectional = bidirectional\n",
    "        \n",
    "        self.embeddings = torch.nn.Embedding(n_tokens, units)\n",
    "        self.attention = BahdanauAttention(units)\n",
    "        self.rnn = torch.nn.GRU(units, units , num_layers=n_stacks, batch_first=True)\n",
    "        self.FC = torch.nn.Linear(units, n_tokens)\n",
    "\n",
    "    def forward(self, enc_outs, enc_h, target=None):\n",
    "        #enc_outs нужен чтобы батч узнать\n",
    "        batch_size = enc_outs.size(0)\n",
    "        dec_inp = torch.empty(batch_size, 1, dtype=torch.long, device='cuda').fill_(tokenizer.bos_token_id)\n",
    "        dec_h, dec_out = enc_h, []\n",
    "        \n",
    "        #цикл генерации токенов, подумой можно ли остановить, если пришел паддинг\n",
    "        out, dec_h = self.apply_rnn(dec_inp, dec_h, enc_outs)\n",
    "        dec_out.append(out)\n",
    "        for i in range(1, MAX_LENGTH-1):\n",
    "            out, dec_h = self.apply_rnn(dec_inp, dec_h, enc_outs)\n",
    "            dec_out.append(out)\n",
    "            dec_inp = target[:, i].unsqueeze(1) if target is not None else out.argmax(dim=-1).detach()\n",
    "            \n",
    "            \n",
    "        dec_out = torch.cat(dec_out, dim=1)\n",
    "        return dec_out\n",
    "\n",
    "    def apply_rnn(self, inp, h, enc_out):\n",
    "        embeds = self.embeddings(inp)\n",
    "        \n",
    "        #query = h.permute(1, 0, 2)\n",
    "        #context, attn_weights = self.attention(query, enc_out)\n",
    "        \n",
    "        #input_gru = torch.cat((embedded, context), dim=2)\n",
    "        \n",
    "        out, h = self.rnn(embeds, h)\n",
    "        out = self.FC(out)\n",
    "\n",
    "        return out, h\n",
    "\n",
    "class Seq2Seq(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, units: int, n_tokens: int, n_stacks: int, bidirectional: bool, name: str):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(units, n_tokens, n_stacks, bidirectional, name+'_encoder').to('cuda')\n",
    "        self.decoder = Decoder(units, n_tokens, n_stacks, name+'_decoder').to('cuda')\n",
    "\n",
    "    def forward(self, text, target=None):\n",
    "        enc_outs, enc_h = self.encoder(text)\n",
    "        dec_out = self.decoder(enc_outs, enc_h, target)\n",
    "        \n",
    "        return dec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebe8ec38-fba7-4168-979e-cc9a8fcdf36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 31, 50257])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(32, tokenizer.vocab_size, 1, False, 'encoder').to('cuda')\n",
    "decoder = Decoder(32, tokenizer.vocab_size, 1, 'decoder').to('cuda')\n",
    "text = next(iter(data))[0][:4]\n",
    "enc_outs, enc_h = encoder(text)\n",
    "dec_out = decoder(enc_outs, enc_h)\n",
    "dec_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8216683-85d9-458e-bfc3-3649a9a362c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 50257])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Seq2Seq(32, tokenizer.vocab_size, 1, False, 'seq2seq')\n",
    "model(text)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee04317-1bbe-4f23-8e1e-595577d35663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(\n",
    "    units: int,\n",
    "    n_tokens: int,\n",
    "    n_labels: int,\n",
    "    n_stacks: int = 1,\n",
    "    bidirectional: bool = False,\n",
    "    name: str | None = None,\n",
    "    cell_type: type[keras.layers.Layer] = keras.layers.LSTMCell\n",
    ") -> keras.Model:\n",
    "    '''Creates a model with RNN architecture for sequence to sequence classification.\n",
    "\n",
    "    Arguments:\n",
    "        units: dimensionality of RNN cells\n",
    "        n_tokens: number of tokens in the tokenizer dictionary\n",
    "        n_labels: number of labels to be predicted\n",
    "        n_stacks: number of RNN cells in the stack (1 -- no stacking)\n",
    "        bidirectional: whether or not the model is bidirectional\n",
    "        name: the model name\n",
    "        cell_type: type of a cell to use, either keras.layers.LSTMCell or keras.layers.GRUCell\n",
    "\n",
    "    Returns:\n",
    "        The model'''\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaf444d-10f3-497d-8cc5-22a86835f433",
   "metadata": {},
   "source": [
    "Try to add attention to your model (for example [additive attention](https://keras.io/api/layers/attention_layers/additive_attention/)), does it perform better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c5d763-e07d-48cd-a539-987a99ad74a7",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2006bfa0-eee3-439d-899d-7612e7a95a90",
   "metadata": {},
   "source": [
    "Train your model using teacher forcing. The idea is that the model predicts the next token that should follow, so one part of the model (called encoder) reads the text and output some state containing information about the text read. The other part of the model (called decoder) reads and already generated text (or in case of the teacher forcing the expected output) and predicts the next token for each one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26530629-8f9b-40cf-82aa-9a0f0b1ca890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, loss_f, dataloader, teacher = None):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(dataloader, start=1):\n",
    "        inp, out_BOS, out_EOS, masking_PAD = batch\n",
    "        decoder_optimizer.zero_grad()\n",
    "        encoder_optimizer.zero_grad()\n",
    "        enc_outs, enc_h = encoder(inp)\n",
    "        seq_pred = decoder(enc_outs, enc_h, out_BOS if teacher else None)\n",
    "        #seq_pred = model(inp, out_BOS if teacher else None) #out_EOS\n",
    "        # loss = loss_f(seq_pred[masking_PAD], \n",
    "        #               out_EOS[masking_PAD])\n",
    "        loss = loss_f(seq_pred.view(-1, seq_pred.size(-1)), \n",
    "                      out_EOS.view(-1))\n",
    "        #return seq_pred, masking_PAD, out_EOS\n",
    "\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % (len(dataloader) // 4) == 0:\n",
    "            print(f'batch{i} cur_loss {running_loss / i}')\n",
    "        \n",
    "    return running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f67cd6d0-870d-41c5-bdee-cc584b9b01f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(model, dataloader, tokenizer, metric, target = None):\n",
    "#     model.to('cuda')\n",
    "#     model.eval()\n",
    "#     running_loss = 0\n",
    "#     running_metric = 0\n",
    "    \n",
    "#     for i, batch in enumerate(dataloader, start=1):\n",
    "#         inp, out_BOS, out_EOS, masking_PAD = batch\n",
    "#         seq_pred = model(inp) #out_EOS\n",
    "\n",
    "#         for seq, y_true, pad in zip(seq_pred, out_EOS, masking_PAD):\n",
    "#             seq = seq.argmax(dim=-1)\n",
    "#             candidates = tokenizer.decode(seq[pad])\n",
    "#             references = torch.Tensor(tokenizer.decode(y_true[pad])).to('cuda')\n",
    "#             metric.update(torch.Tensor(\"I wouldn't be able to solve the problem, even if I tried.\").to('cuda'), [references])\n",
    "        \n",
    "#         loss = loss_f(seq_pred.view(-1, seq_pred.size(-1)), \n",
    "#                       out_EOS.view(-1))\n",
    "#         running_loss += loss.item()\n",
    "        \n",
    "#         #if i % (len(dataloader) // 4) == 0:\n",
    "#         #    print(f'batch{i} cur_loss {running_loss / i}')\n",
    "#     print(metric.device)\n",
    "#     return running_loss / len(dataloader), metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "708ffd4c-4c15-408d-93d4-60e4cb519ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch859 cur_loss 2.4148673558818032\n",
      "batch1718 cur_loss 1.9339274556589625\n",
      "batch2577 cur_loss 1.7446473183722564\n",
      "batch3436 cur_loss 1.63477461530943\n",
      "1.634409904705296\n",
      "batch859 cur_loss 1.2631204627861938\n",
      "batch1718 cur_loss 1.2475239604329897\n",
      "batch2577 cur_loss 1.234690229144261\n",
      "batch3436 cur_loss 1.2244876186874054\n",
      "1.2243502313191545\n",
      "Teacher forcing is OVER\n",
      "batch859 cur_loss 1.351288315025836\n",
      "batch1718 cur_loss 1.3348028373940304\n",
      "batch2577 cur_loss 1.3234091381902697\n",
      "batch3436 cur_loss 1.3182760662680038\n",
      "1.318225599021612\n",
      "batch859 cur_loss 1.2891442310823966\n",
      "batch1718 cur_loss 1.2833537691130765\n",
      "batch2577 cur_loss 1.2811826459650018\n",
      "batch3436 cur_loss 1.2791795703182398\n",
      "1.2791822479020096\n",
      "batch859 cur_loss 1.260246178879033\n",
      "batch1718 cur_loss 1.2609404221402614\n",
      "batch2577 cur_loss 1.260316596921211\n",
      "batch3436 cur_loss 1.2594918812320453\n",
      "1.2594138697920652\n",
      "CPU times: user 14min, sys: 985 ms, total: 14min 1s\n",
      "Wall time: 14min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EPOCHS = 5\n",
    "#model = Seq2Seq(32, tokenizer.vocab_size, 3, True, 'seq2seq')\n",
    "encoder = Encoder(32, tokenizer.vocab_size, 1, False, 'encoder').to('cuda')\n",
    "decoder = Decoder(32, tokenizer.vocab_size, 1, False, 'decoder').to('cuda')\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.001)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001)\n",
    "\n",
    "loss_f = torch.nn.CrossEntropyLoss()\n",
    "teacher = True\n",
    "for epoch in range(EPOCHS):\n",
    "    #metric = BLEUScore(n_gram=3, device='cuda')\n",
    "    \n",
    "    if epoch == 2:\n",
    "        teacher = False\n",
    "        print(f'Teacher forcing is OVER')\n",
    "    loss = train(encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, loss_f, data, teacher = teacher)\n",
    "    print(loss)\n",
    "#score = evaluate(model, data_test, tokenizer, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96e4a0-7c9c-4d1c-97f6-a8d3846b4fde",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7f3475-22b9-47b4-824c-7ebc758b75a2",
   "metadata": {},
   "source": [
    "Make a function for text translation. Translate some text and evaluate model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602f831b-9ddd-4070-ba8f-b7b16bc26463",
   "metadata": {},
   "source": [
    "Take note that your model is set for training. During the inference process you will have to use parts of the model independently (including the RNN cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a32f4dc-3c53-426d-bf67-35b1efe2bbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(\n",
    "    text: str,\n",
    "    tokenizer: Tokenizer,\n",
    "    model: keras.Model,\n",
    "    max_len: int = 20\n",
    ") -> str:\n",
    "    '''Predicts `text`translation using the `model`.\n",
    "\n",
    "    Arguments:\n",
    "        text: text to be translated\n",
    "        tokenizer: tokenizer to use\n",
    "        model: model ot use\n",
    "        max_len: maximum length of the prediction (in tokens)\n",
    "\n",
    "    Returns:\n",
    "        tranlated text'''\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d7cc2f-028c-424d-b9fd-a1fbf518c4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
