{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fe57d51-28e4-481c-af01-a793894c8481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/09/2025 02:33:33 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "{'dynamic_thresholding_ratio', 'timestep_spacing', 'thresholding', 'clip_sample_range', 'variance_type', 'prediction_type', 'rescale_betas_zero_snr', 'sample_max_value'} was not found in config. Values will be initialized to default values.\n",
      "{'scaling_factor', 'mid_block_add_attention', 'shift_factor', 'force_upcast', 'use_post_quant_conv', 'latents_mean', 'use_quant_conv', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "{'mid_block_type', 'time_embedding_act_fn', 'reverse_transformer_layers_per_block', 'only_cross_attention', 'encoder_hid_dim', 'class_embeddings_concat', 'time_cond_proj_dim', 'encoder_hid_dim_type', 'resnet_out_scale_factor', 'resnet_time_scale_shift', 'timestep_post_act', 'upcast_attention', 'dropout', 'attention_type', 'addition_embed_type', 'transformer_layers_per_block', 'addition_time_embed_dim', 'class_embed_type', 'time_embedding_type', 'mid_block_only_cross_attention', 'conv_in_kernel', 'resnet_skip_time_act', 'num_class_embeds', 'dual_cross_attention', 'conv_out_kernel', 'time_embedding_dim', 'num_attention_heads', 'projection_class_embeddings_input_dim', 'use_linear_projection', 'cross_attention_norm', 'addition_embed_type_num_heads'} was not found in config. Values will be initialized to default values.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "02/09/2025 02:33:58 - INFO - __main__ - ***** Running training *****\n",
      "02/09/2025 02:33:58 - INFO - __main__ -   Num examples = 700\n",
      "02/09/2025 02:33:58 - INFO - __main__ -   Num Epochs = 18\n",
      "02/09/2025 02:33:58 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "02/09/2025 02:33:58 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "02/09/2025 02:33:58 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
      "02/09/2025 02:33:58 - INFO - __main__ -   Total optimization steps = 3000\n",
      "Steps:  17%|█▋        | 500/3000 [10:59<54:52,  1.32s/it, loss=0.0428, lr=0.002]02/09/2025 02:44:57 - INFO - __main__ - Saving embeddings\n",
      "02/09/2025 02:44:57 - INFO - accelerate.accelerator - Saving current state to ./textual_output\\/checkpoint-500\n",
      "02/09/2025 02:44:57 - INFO - accelerate.checkpointing - Model weights saved in textual_output\\/checkpoint-500/model.safetensors\n",
      "02/09/2025 02:44:58 - INFO - accelerate.checkpointing - Optimizer state saved in textual_output\\/checkpoint-500/optimizer.bin\n",
      "02/09/2025 02:44:58 - INFO - accelerate.checkpointing - Scheduler state saved in textual_output\\/checkpoint-500/scheduler.bin\n",
      "02/09/2025 02:44:58 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in textual_output\\/checkpoint-500/sampler.bin\n",
      "02/09/2025 02:44:58 - INFO - accelerate.checkpointing - Gradient scaler state saved in textual_output\\/checkpoint-500/scaler.pt\n",
      "02/09/2025 02:44:58 - INFO - accelerate.checkpointing - Random states saved in textual_output\\/checkpoint-500/random_states_0.pkl\n",
      "02/09/2025 02:44:58 - INFO - __main__ - Saved state to ./textual_output\\/checkpoint-500\n",
      "Steps:  33%|███      | 1000/3000 [21:57<43:53,  1.32s/it, loss=0.0411, lr=0.002]02/09/2025 02:55:56 - INFO - __main__ - Saving embeddings\n",
      "02/09/2025 02:55:56 - INFO - accelerate.accelerator - Saving current state to ./textual_output\\/checkpoint-1000\n",
      "02/09/2025 02:55:56 - INFO - accelerate.checkpointing - Model weights saved in textual_output\\/checkpoint-1000/model.safetensors\n",
      "02/09/2025 02:55:56 - INFO - accelerate.checkpointing - Optimizer state saved in textual_output\\/checkpoint-1000/optimizer.bin\n",
      "02/09/2025 02:55:56 - INFO - accelerate.checkpointing - Scheduler state saved in textual_output\\/checkpoint-1000/scheduler.bin\n",
      "02/09/2025 02:55:56 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in textual_output\\/checkpoint-1000/sampler.bin\n",
      "02/09/2025 02:55:56 - INFO - accelerate.checkpointing - Gradient scaler state saved in textual_output\\/checkpoint-1000/scaler.pt\n",
      "02/09/2025 02:55:56 - INFO - accelerate.checkpointing - Random states saved in textual_output\\/checkpoint-1000/random_states_0.pkl\n",
      "02/09/2025 02:55:56 - INFO - __main__ - Saved state to ./textual_output\\/checkpoint-1000\n",
      "Steps:  50%|████    | 1500/3000 [32:56<32:48,  1.31s/it, loss=0.00645, lr=0.002]02/09/2025 03:06:54 - INFO - __main__ - Saving embeddings\n",
      "02/09/2025 03:06:54 - INFO - accelerate.accelerator - Saving current state to ./textual_output\\/checkpoint-1500\n",
      "02/09/2025 03:06:55 - INFO - accelerate.checkpointing - Model weights saved in textual_output\\/checkpoint-1500/model.safetensors\n",
      "02/09/2025 03:06:55 - INFO - accelerate.checkpointing - Optimizer state saved in textual_output\\/checkpoint-1500/optimizer.bin\n",
      "02/09/2025 03:06:55 - INFO - accelerate.checkpointing - Scheduler state saved in textual_output\\/checkpoint-1500/scheduler.bin\n",
      "02/09/2025 03:06:55 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in textual_output\\/checkpoint-1500/sampler.bin\n",
      "02/09/2025 03:06:55 - INFO - accelerate.checkpointing - Gradient scaler state saved in textual_output\\/checkpoint-1500/scaler.pt\n",
      "02/09/2025 03:06:55 - INFO - accelerate.checkpointing - Random states saved in textual_output\\/checkpoint-1500/random_states_0.pkl\n",
      "02/09/2025 03:06:55 - INFO - __main__ - Saved state to ./textual_output\\/checkpoint-1500\n",
      "Steps:  67%|██████   | 2000/3000 [43:55<21:46,  1.31s/it, loss=0.0708, lr=0.002]02/09/2025 03:17:53 - INFO - __main__ - Saving embeddings\n",
      "02/09/2025 03:17:53 - INFO - accelerate.accelerator - Saving current state to ./textual_output\\/checkpoint-2000\n",
      "02/09/2025 03:17:53 - INFO - accelerate.checkpointing - Model weights saved in textual_output\\/checkpoint-2000/model.safetensors\n",
      "02/09/2025 03:17:54 - INFO - accelerate.checkpointing - Optimizer state saved in textual_output\\/checkpoint-2000/optimizer.bin\n",
      "02/09/2025 03:17:54 - INFO - accelerate.checkpointing - Scheduler state saved in textual_output\\/checkpoint-2000/scheduler.bin\n",
      "02/09/2025 03:17:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in textual_output\\/checkpoint-2000/sampler.bin\n",
      "02/09/2025 03:17:54 - INFO - accelerate.checkpointing - Gradient scaler state saved in textual_output\\/checkpoint-2000/scaler.pt\n",
      "02/09/2025 03:17:54 - INFO - accelerate.checkpointing - Random states saved in textual_output\\/checkpoint-2000/random_states_0.pkl\n",
      "02/09/2025 03:17:54 - INFO - __main__ - Saved state to ./textual_output\\/checkpoint-2000\n",
      "Steps:  83%|██████▋ | 2500/3000 [54:54<10:56,  1.31s/it, loss=0.00492, lr=0.002]02/09/2025 03:28:52 - INFO - __main__ - Saving embeddings\n",
      "02/09/2025 03:28:52 - INFO - accelerate.accelerator - Saving current state to ./textual_output\\/checkpoint-2500\n",
      "02/09/2025 03:28:52 - INFO - accelerate.checkpointing - Model weights saved in textual_output\\/checkpoint-2500/model.safetensors\n",
      "02/09/2025 03:28:52 - INFO - accelerate.checkpointing - Optimizer state saved in textual_output\\/checkpoint-2500/optimizer.bin\n",
      "02/09/2025 03:28:52 - INFO - accelerate.checkpointing - Scheduler state saved in textual_output\\/checkpoint-2500/scheduler.bin\n",
      "02/09/2025 03:28:52 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in textual_output\\/checkpoint-2500/sampler.bin\n",
      "02/09/2025 03:28:52 - INFO - accelerate.checkpointing - Gradient scaler state saved in textual_output\\/checkpoint-2500/scaler.pt\n",
      "02/09/2025 03:28:52 - INFO - accelerate.checkpointing - Random states saved in textual_output\\/checkpoint-2500/random_states_0.pkl\n",
      "02/09/2025 03:28:52 - INFO - __main__ - Saved state to ./textual_output\\/checkpoint-2500\n",
      "Steps: 100%|██████| 3000/3000 [1:05:51<00:00,  1.32s/it, loss=0.00373, lr=0.002]02/09/2025 03:39:49 - INFO - __main__ - Saving embeddings\n",
      "02/09/2025 03:39:49 - INFO - accelerate.accelerator - Saving current state to ./textual_output\\/checkpoint-3000\n",
      "02/09/2025 03:39:50 - INFO - accelerate.checkpointing - Model weights saved in textual_output\\/checkpoint-3000/model.safetensors\n",
      "02/09/2025 03:39:50 - INFO - accelerate.checkpointing - Optimizer state saved in textual_output\\/checkpoint-3000/optimizer.bin\n",
      "02/09/2025 03:39:50 - INFO - accelerate.checkpointing - Scheduler state saved in textual_output\\/checkpoint-3000/scheduler.bin\n",
      "02/09/2025 03:39:50 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in textual_output\\/checkpoint-3000/sampler.bin\n",
      "02/09/2025 03:39:50 - INFO - accelerate.checkpointing - Gradient scaler state saved in textual_output\\/checkpoint-3000/scaler.pt\n",
      "02/09/2025 03:39:50 - INFO - accelerate.checkpointing - Random states saved in textual_output\\/checkpoint-3000/random_states_0.pkl\n",
      "02/09/2025 03:39:50 - INFO - __main__ - Saved state to ./textual_output\\/checkpoint-3000\n",
      "Steps: 100%|██████| 3000/3000 [1:05:52<00:00,  1.32s/it, loss=0.00855, lr=0.002]02/09/2025 03:39:50 - INFO - __main__ - Saving embeddings\n",
      "Steps: 100%|██████| 3000/3000 [1:05:52<00:00,  1.32s/it, loss=0.00855, lr=0.002]\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch textual_inversion.py \\\n",
    "  --mixed_precision=\"fp16\"\\\n",
    "  --pretrained_model_name_or_path=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\\\n",
    "  --train_data_dir=\"dataset\"\\\n",
    "  --learnable_property=\"object\" \\\n",
    "  --placeholder_token=\"<web-page>\" \\\n",
    "  --initializer_token=\"web\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --max_train_steps=3000 \\\n",
    "  --learning_rate=5.0e-04 \\\n",
    "  --scale_lr \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --output_dir=\"./textual_output\"\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e801ae4a-c4d7-4882-bacc-a0b0e6d7a52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|████████████████████████████████████████████████| 7/7 [00:00<00:00,  7.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 500/500 [01:20<00:00,  6.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline.load_textual_inversion(\"textual_output\")\n",
    "image = pipeline(\"web page of cats <web-page>\", num_inference_steps=500).images[0]\n",
    "image.save(\"funco.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e83a818-64e5-4fba-8b97-bb3eb8b6d2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/09/2025 11:51:39 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "{'rescale_betas_zero_snr', 'sample_max_value', 'variance_type', 'timestep_spacing', 'dynamic_thresholding_ratio', 'prediction_type', 'thresholding', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
      "{'use_post_quant_conv', 'scaling_factor', 'use_quant_conv', 'latents_std', 'force_upcast', 'shift_factor', 'mid_block_add_attention', 'latents_mean'} was not found in config. Values will be initialized to default values.\n",
      "{'dropout', 'resnet_out_scale_factor', 'transformer_layers_per_block', 'time_embedding_type', 'time_cond_proj_dim', 'num_class_embeds', 'mid_block_only_cross_attention', 'only_cross_attention', 'mid_block_type', 'encoder_hid_dim_type', 'resnet_time_scale_shift', 'attention_type', 'time_embedding_act_fn', 'use_linear_projection', 'num_attention_heads', 'resnet_skip_time_act', 'conv_out_kernel', 'addition_time_embed_dim', 'upcast_attention', 'addition_embed_type', 'cross_attention_norm', 'dual_cross_attention', 'reverse_transformer_layers_per_block', 'timestep_post_act', 'conv_in_kernel', 'encoder_hid_dim', 'class_embeddings_concat', 'addition_embed_type_num_heads', 'class_embed_type', 'time_embedding_dim', 'projection_class_embeddings_input_dim'} was not found in config. Values will be initialized to default values.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "02/09/2025 11:51:43 - INFO - __main__ - ***** Running training *****\n",
      "02/09/2025 11:51:43 - INFO - __main__ -   Num examples = 1000\n",
      "02/09/2025 11:51:43 - INFO - __main__ -   Num Epochs = 2\n",
      "02/09/2025 11:51:43 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "02/09/2025 11:51:43 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "02/09/2025 11:51:43 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
      "02/09/2025 11:51:43 - INFO - __main__ -   Total optimization steps = 500\n",
      "Steps: 100%|███████████| 500/500 [11:06<00:00,  1.35s/it, loss=0.0229, lr=0.002]02/09/2025 12:02:50 - INFO - __main__ - Saving embeddings\n",
      "02/09/2025 12:02:50 - INFO - accelerate.accelerator - Saving current state to ./burzuminator_output\\/checkpoint-500\n",
      "02/09/2025 12:02:50 - INFO - accelerate.checkpointing - Model weights saved in burzuminator_output\\/checkpoint-500/model.safetensors\n",
      "02/09/2025 12:02:50 - INFO - accelerate.checkpointing - Optimizer state saved in burzuminator_output\\/checkpoint-500/optimizer.bin\n",
      "02/09/2025 12:02:50 - INFO - accelerate.checkpointing - Scheduler state saved in burzuminator_output\\/checkpoint-500/scheduler.bin\n",
      "02/09/2025 12:02:50 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in burzuminator_output\\/checkpoint-500/sampler.bin\n",
      "02/09/2025 12:02:50 - INFO - accelerate.checkpointing - Gradient scaler state saved in burzuminator_output\\/checkpoint-500/scaler.pt\n",
      "02/09/2025 12:02:50 - INFO - accelerate.checkpointing - Random states saved in burzuminator_output\\/checkpoint-500/random_states_0.pkl\n",
      "02/09/2025 12:02:50 - INFO - __main__ - Saved state to ./burzuminator_output\\/checkpoint-500\n",
      "Steps: 100%|██████████| 500/500 [11:07<00:00,  1.35s/it, loss=0.00644, lr=0.002]02/09/2025 12:02:50 - INFO - __main__ - Saving embeddings\n",
      "Steps: 100%|██████████| 500/500 [11:07<00:00,  1.34s/it, loss=0.00644, lr=0.002]\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch textual_inversion.py \\\n",
    "  --mixed_precision=\"fp16\"\\\n",
    "  --pretrained_model_name_or_path=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\\\n",
    "  --train_data_dir=\"dataset_burzuminator\"\\\n",
    "  --learnable_property=\"object\" \\\n",
    "  --placeholder_token=\"<burzuminator>\" \\\n",
    "  --initializer_token=\"black\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --max_train_steps=500 \\\n",
    "  --learning_rate=5.0e-04 \\\n",
    "  --scale_lr \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --output_dir=\"./burzuminator_output\"\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bf7590f-3526-4773-8670-2ad6ebe42ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|████████████████████████████████████████████████| 7/7 [00:00<00:00,  7.94it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 500/500 [01:21<00:00,  6.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline.load_textual_inversion(\"burzuminator_output\")\n",
    "image = pipeline(\"Jesus cross god <burzuminator>\", num_inference_steps=500).images[0]\n",
    "image.save(\"burzuminator_jesus.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8573693a-db84-4d80-85a4-f52d534ff153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
