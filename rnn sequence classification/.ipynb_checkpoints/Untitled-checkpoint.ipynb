{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "1f936187-5388-4653-ae26-b380a44fea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hugging\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "#from torchsummary import summary\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0271b4e7-71e8-4ec6-ad27-2cfe37ea67d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 3050 is available.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a631fca-54be-4723-a17d-1008b4d85eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6aee82-3458-454d-8fd0-c8c6f3907639",
   "metadata": {},
   "source": [
    "# Processing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02def515-5f0c-4c59-9a86-723a243ca4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset('google-research-datasets/go_emotions', 'raw')\n",
    "CLASSES = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "674d27c3-0462-4fe6-9b3c-4f0db79aef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_from_ds(ds: datasets.dataset_dict, CLASSES: list[str], split: str ='train') -> tuple[list[str], list[int]]:\n",
    "    y = []\n",
    "    for c in CLASSES:\n",
    "        y.append(ds[split][c])\n",
    "    return ds[split]['text'], np.argmax(y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db5bbc46-3857-4d2b-a25d-fbce0ec69903",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_X_y_from_ds(ds, CLASSES)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b5b6fb-de06-49fa-9ceb-0f9b66ebed33",
   "metadata": {},
   "source": [
    "# Preparing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "022a9f42-dbf9-4b01-bef8-d8ea344e2135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bob1ch/Рабочий стол/NN-NLP/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "69e2acc0-3b12-494c-9db3-0df374425974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, num_layers, classes):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        #Хочет LxBxE_dims\n",
    "        #При batch_first=True хочет BxLxE_dims\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.FC = nn.Linear(hidden_dim, classes)\n",
    "\n",
    "                        #n_layers x B x H_out\n",
    "    \n",
    "    def forward(self, text):\n",
    "        # text B x len\n",
    "        embeddings = self.embeddings(text) # B x len x H(embedding_dim)\n",
    "        inp = embeddings.view(-1, len(text), self.embedding_dim) # L x B x H_in так нужно сделать, чтобы в LSTM кинуть\n",
    "        #self.hidden = self.init_hidden_state(text) #h_0 и c_0 n_layers x B x H_out\n",
    "\n",
    "        lstm_out, (h, c) = self.lstm(inp) #out: L x B x Hout h_t: n_l x B x H_out\n",
    "        out = lstm_out[-1]\n",
    "\n",
    "        \n",
    "        # In each timestep of an LSTM the input goes through a simple neural network and the output gets passed to the next timestep. The output out of function\n",
    "        # out, (ht, ct) = self.lstm_nets(X)\n",
    "        # contains a list of ALL outputs (i.e the output of the neural networks of every timestep). Yet, in classification, you mostly only really care about the LAST output. You can get it like this:\n",
    "        # out = out[:, -1]\n",
    "        # https://stackoverflow.com/questions/72667646/how-to-connect-a-lstm-layer-to-a-linear-layer-in-pytorch\n",
    "        \n",
    "        out = self.FC(out) \n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "2da1c470-18d8-46c9-81f8-bf6bf5c749f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#E = 32\n",
    "#H = 128\n",
    "#len(tokenz) = 64\n",
    "rnn = RNN(embedding_dim=32, \n",
    "          hidden_dim=32, \n",
    "          vocab_size=502576, \n",
    "          num_layers=5, \n",
    "          classes=28).to('cuda')\n",
    "\n",
    "text = tokenizer(X[0:2], return_tensors='pt', padding='max_length', max_length=32, truncation=True)\n",
    "x = rnn(text['input_ids'].to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "c700bd18-3756-4647-be9f-88e7d0307def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1848,  0.2165, -0.1298, -0.1484,  0.1609,  0.0748, -0.1140, -0.1392,\n",
       "          0.0302, -0.0084,  0.1534,  0.0493, -0.1330, -0.0298,  0.0810, -0.0614,\n",
       "          0.1113,  0.0812, -0.2749,  0.0076, -0.1517, -0.0683, -0.1889, -0.1362,\n",
       "         -0.1175, -0.1181,  0.0526,  0.0017],\n",
       "        [ 0.1850,  0.2166, -0.1299, -0.1480,  0.1608,  0.0748, -0.1138, -0.1389,\n",
       "          0.0305, -0.0088,  0.1536,  0.0497, -0.1330, -0.0295,  0.0813, -0.0617,\n",
       "          0.1114,  0.0808, -0.2746,  0.0078, -0.1518, -0.0681, -0.1892, -0.1360,\n",
       "         -0.1173, -0.1175,  0.0528,  0.0016]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2419dc-20af-4980-86c8-8cfbaee940cf",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "b77cdcb2-d1fd-46bc-b3c1-f934e75398b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_dim, hidden_dim, vocab_size, num_layers, classes\n",
    "model = RNN(embedding_dim=64, \n",
    "            hidden_dim=64, \n",
    "            vocab_size=502576, \n",
    "            num_layers=5, \n",
    "            classes=28)\n",
    "model.to('cuda')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "7c1ba63f-7d0f-47e6-8e5c-a4be349bd998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = tqdm.trange(2, leave=False)\n",
    "# for _ in epochs:\n",
    "#     epoch = tqdm.trange(0, len(X_train), 100, leave=False)\n",
    "#     for i in epoch:\n",
    "#         model.zero_grad() #очистить градиенты\n",
    "\n",
    "#         X, y = X_train[i: i+100], y_train[i: i+100]\n",
    "#         X = tokenizer(X, return_tensors='pt', padding='max_length', max_length=64, truncation=True)['input_ids']\n",
    "#         y_pred = model(X.to('cuda'))\n",
    "#         y = torch.tensor(y).to('cuda')\n",
    "#         loss = loss_f(y_pred, y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step\n",
    "\n",
    "#         #epoch.set_description(f'loss: {torch.mean(loss):.3f}')\n",
    "#     #print(evaluate(X_test, y_test, model, 100))\n",
    "#     acc = 0\n",
    "#     for i in range(0, len(X_test), 100):\n",
    "#         X, y = X_test[i: i+100], y_test[i: i+100]\n",
    "#         X = tokenizer(X, return_tensors='pt', padding='max_length', max_length=64, truncation=True)['input_ids']\n",
    "#         y_pred = torch.argmax(model(X.to('cuda')), dim=1)\n",
    "#         y = torch.tensor(y).to('cuda')\n",
    "#         acc += torch.sum(y_pred == y) / len(y_pred)\n",
    "#     acc /= len(range(0, len(ds), 100))\n",
    "#     print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "458e8f26-2dac-4cbc-a63c-1ed4514eef1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "        27, 27], device='cuda:0')"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_i = tokenizer(X_train[50: 70].tolist(), return_tensors='pt', padding='max_length', max_length=64, truncation=True)['input_ids']\n",
    "torch.argmax(model(X_i.to('cuda')), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "1062f405-79c5-4272-86a8-f788b846936d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.006708536148071289\n",
      "[1, 20001] loss: 0.227436439037323\n",
      "[1, 40001] loss: 0.22370239734649658\n",
      "[1, 60001] loss: 0.22422976636886596\n",
      "[1, 80001] loss: 0.2235818691253662\n",
      "[1, 100001] loss: 0.22385192584991456\n",
      "[1, 120001] loss: 0.2246911940574646\n",
      "[1, 140001] loss: 0.22390100193023682\n",
      "----------------------------------------\n",
      "Train accuracy 26%\n",
      "Test accuracy 25%\n",
      "----------------------------------------\n",
      "[2,     1] loss: 0.0056212182044982914\n",
      "[2, 20001] loss: 0.22303049039840697\n",
      "[2, 40001] loss: 0.22495844221115113\n",
      "[2, 60001] loss: 0.2229218020439148\n",
      "[2, 80001] loss: 0.22483359813690185\n",
      "[2, 100001] loss: 0.2242973985671997\n",
      "[2, 120001] loss: 0.22355601835250855\n",
      "[2, 140001] loss: 0.22485375928878784\n",
      "----------------------------------------\n",
      "Train accuracy 26%\n",
      "Test accuracy 25%\n",
      "----------------------------------------\n",
      "[3,     1] loss: 0.005762024879455567\n",
      "[3, 20001] loss: 0.22305150318145753\n",
      "[3, 40001] loss: 0.22294979572296142\n",
      "[3, 60001] loss: 0.22356168508529664\n",
      "[3, 80001] loss: 0.22339254426956176\n",
      "[3, 100001] loss: 0.22350958013534547\n",
      "[3, 120001] loss: 0.22466628742218017\n",
      "[3, 140001] loss: 0.22448169803619386\n",
      "----------------------------------------\n",
      "Train accuracy 26%\n",
      "Test accuracy 25%\n",
      "----------------------------------------\n",
      "[4,     1] loss: 0.005558137893676758\n",
      "[4, 20001] loss: 0.2236487603187561\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[527], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# print statistics\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20_000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:    \u001b[38;5;66;03m# print every 10000 mini-batches\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m5d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m500\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_acc = []\n",
    "val_acc = []\n",
    "n_epochs = 10\n",
    "rng = np.random.default_rng()\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, len(X_train), 500):\n",
    "        idx = rng.choice(len(X_train), size=500, replace=False)\n",
    "        X_i, y_i = X_train[idx], y_train[idx]\n",
    "        X_i = tokenizer(X_i.tolist(), return_tensors='pt', padding='max_length', max_length=64, truncation=True)['input_ids']\n",
    "        y_i = torch.Tensor(y_i).type(torch.LongTensor).to('cuda')\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        y_pred = model(X_i.to('cuda'))\n",
    "        loss = criterion(y_pred, y_i)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20_000 == 0:    # print every 10000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 500}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    train_acc.append(get_acc(X_train.tolist(), y_train, 500, tokenizer, model))\n",
    "    val_acc.append(get_acc(X_test, y_test, 500, tokenizer, model))\n",
    "    print('-'*40)\n",
    "    print(f'Train accuracy {train_acc[-1]}%')\n",
    "    print(f'Test accuracy {val_acc[-1]}%')\n",
    "    print('-'*40)\n",
    "print('Finished Training')\n",
    "\n",
    "plt.plot(range(n_epochs), train_acc, c='r')\n",
    "plt.plot(range(n_epochs), val_acc, c='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "db36a121-5122-4d26-a021-de2635d0cebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (word_embeddings): Embedding(502576, 128)\n",
      "  (lstm): LSTM(128, 256, num_layers=3)\n",
      "  (FC): Linear(in_features=256, out_features=28, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "1ab9d92f-1db0-4699-b8b1-621d1165288f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 147857 sentences: 16 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X_train), 500):\n",
    "        X_i, y_i = X_train[i: i+500], y_train[i: i+500]\n",
    "        X_i = tokenizer(X_i, return_tensors='pt', padding='max_length', max_length=64, truncation=True)['input_ids']\n",
    "        y_i = torch.Tensor(y_i).to('cuda')\n",
    "        \n",
    "        y_pred = torch.argmax(model(X_i.to('cuda')), dim=1)\n",
    "        total += y_i.size(0)\n",
    "        correct += (y_pred == y_i).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the {len(X_train)} sentences: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "3c16f642-abf7-4a96-8e52-08444f6ccc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9943\n",
      "63368\n",
      "Accuracy of the network on the 63368 sentences: 15 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X_test), 500):\n",
    "        X_i, y_i = X_test[i: i+500], y_test[i: i+500]\n",
    "        X_i = tokenizer(X_i, return_tensors='pt', padding='max_length', max_length=64, truncation=True)['input_ids']\n",
    "        y_i = torch.Tensor(y_i).to('cuda')\n",
    "        \n",
    "        y_pred = torch.argmax(model(X_i.to('cuda')), dim=1)\n",
    "        total += y_i.size(0)\n",
    "        correct += (y_pred == y_i).sum().item()\n",
    "print(correct)\n",
    "print(total)\n",
    "print(f'Accuracy of the network on the {len(X_test)} sentences: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "1c094fc6-a530-4c49-868a-f689774ffb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False,  True, False, False, False,  True,\n",
       "         True, False, False, False, False, False, False, False, False, False,\n",
       "        False, False,  True, False,  True, False,  True,  True, False, False,\n",
       "        False, False, False, False, False,  True, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False,  True, False, False, False, False, False, False, False,\n",
       "         True, False, False, False, False, False, False, False,  True,  True,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False,  True, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "         True, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False,  True, False, False,  True,  True, False, False, False,\n",
       "        False, False, False,  True, False, False, False, False, False, False,\n",
       "         True, False, False, False,  True,  True, False,  True, False, False,\n",
       "         True, False,  True, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False,  True, False, False, False,\n",
       "         True, False,  True, False, False, False, False, False, False, False,\n",
       "        False,  True, False, False, False,  True, False, False, False, False,\n",
       "         True, False, False,  True, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "         True, False, False, False, False, False, False, False, False, False,\n",
       "        False,  True, False, False, False, False, False,  True,  True, False,\n",
       "        False, False, False, False, False, False,  True,  True, False, False,\n",
       "        False, False, False, False, False,  True,  True, False,  True, False,\n",
       "        False, False, False, False, False,  True,  True, False, False, False,\n",
       "        False, False, False, False, False, False, False,  True, False, False,\n",
       "        False, False,  True, False,  True, False, False, False,  True,  True,\n",
       "        False, False, False,  True,  True, False, False, False, False, False,\n",
       "        False,  True,  True,  True, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False,  True, False, False, False,\n",
       "         True, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False,  True],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred == y_i)#.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1788a4b3-cbe5-4302-be17-ec930e88633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(X, y, step, tokenizer, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X), step):\n",
    "            X_i, y_i = X[i: i+step], y[i: i+step]\n",
    "            X_i = tokenizer(X_i, return_tensors='pt', padding='max_length', max_length=64, truncation=True)['input_ids']\n",
    "            y_i = torch.Tensor(y_i).to('cuda')\n",
    "            \n",
    "            y_pred = torch.argmax(model(X_i.to('cuda')), dim=1)\n",
    "            total += y_i.size(0)\n",
    "            correct += (y_pred == y_i).sum().item()\n",
    "    \n",
    "    #print(f'Accuracy of the network on the {len(X_test)} sentences: {100 * correct // total} %')\n",
    "    \n",
    "    return 100 * correct // total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af83418-0e88-4140-81a1-0534e9509580",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c79cf8c-1d7d-4e8a-9afe-3b619a3ce16a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
