{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2dbe8ed-bc36-4ce8-9fd1-22866ca3ad2e",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* ~~надо придумать как батчи пихнуть (var_text)~~\n",
    "* ~~\"pack\" the sequences in PyTorch~~\n",
    "* Придумать метрику"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4b7aaf-d09b-4b64-91e6-dee2976ce379",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f89b44-fede-422c-b525-9a267aad4d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bob1ch/Рабочий стол/NN-NLP/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#import keras\n",
    "import datasets\n",
    "import numpy as np\n",
    "import transformers\n",
    "import sklearn.metrics\n",
    "#import tensorflow as tf\n",
    "import tqdm.notebook as tqdm\n",
    "import sklearn.model_selection\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4e64b0-7eef-4af2-9906-44c8cdb86c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch\n",
    "import torch\n",
    "from torcheval.metrics import MultilabelAUPRC\n",
    "from torchmetrics.classification import MultilabelAUROC\n",
    "#from torchsummary import summary\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d400cd26-730e-4c4e-aaed-5987f78ef5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfb9f8d-1cf5-4954-8b70-18d47dee46e3",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8cc56a-5c94-4134-902d-e74edf1414aa",
   "metadata": {},
   "source": [
    "Load the dataset (we will be using [go_emotions](https://huggingface.co/datasets/google-research-datasets/go_emotions)). Pretokenize data or make a loader that tokenizes the sentenses as you iterate through the dataset. Implement two datasets: variable and fixed sentence length (in tokens). Don't forget to split the dataset into train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fea15226-0e61-4fcd-a188-d6330e545465",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('google-research-datasets/go_emotions', name='raw', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b144fe06-9838-4812-9e19-9b74e903bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n",
    "    'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear',\n",
    "    'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
    "    'remorse', 'sadness', 'surprise', 'neutral'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "682d6798-62fe-4d5c-8b8b-99394df534ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bob1ch/Рабочий стол/NN-NLP/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddcc3ba9-fb2c-40b2-af4b-db4595895756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This person is the smartest person to play town of salem literally 999999999999999999999999999999999999999999999999999999999999999999999999999999999999999991000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001234567898765432345676543345678987654345678909876543234567898765432345678909876543234567898765432345678987654323456787654345676543456543456434543434343434323456765434567654323454323456543345678987654323456789876565656565656565656565656565454545654565454323456765432345678765456 IQ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1212,  1048,   318,   262, 44730,  1048,   284,   711,  3240,   286,\n",
       "          3664,   368,  7360,   860, 24214, 24214, 24214, 24214, 24214, 24214,\n",
       "         24214, 24214, 24214, 24214, 24214, 24214, 24214, 24214, 24214, 24214,\n",
       "         24214, 24214, 24214, 24214, 24214, 24214,    16, 25645, 25645, 25645,\n",
       "         25645, 25645, 25645, 25645, 25645, 25645, 25645, 25645, 25645, 25645,\n",
       "         25645, 25645, 25645,  8269,   405, 10163,  2231, 30924,  4089, 29143,\n",
       "          3559,  1954,  2231,  3134, 39111,  2091,  2231, 30924,  4089, 29143,\n",
       "          3559,  2231,  3134,  4531,  2931,  5774,  2996,  3559,  1954,  2231,\n",
       "         30924,  4089, 29143,  3559,  1954,  2231,  3134,  4531,  2931,  5774,\n",
       "          2996,  3559,  1954,  2231, 30924,  4089, 29143,  3559,  1954,  2231,\n",
       "         30924,  4089, 29143,  3559,  1954,  2231,  3134,  5774,  2996,  3559,\n",
       "          2231,  3134,  2996,  3559,  2231,  2996,  3559,  2231,  2414, 27712,\n",
       "         47101,  2682,  2682,  2682, 32118,  1954,  2231,  3134,  2996,  3559,\n",
       "          2231,  3134,  2996,  3559,  1954,  2231,  3559,  1954,  2231, 39111,\n",
       "          2091,  2231, 30924,  4089, 29143,  3559,  1954,  2231, 30924,  4089,\n",
       "         29143,  2996,  2996,  2996,  2996,  2996,  2996,  2996,  2996,  2996,\n",
       "          2996,  2996,  2996,  2996,  2231,  2231,  2231,  2996,  2231,  2996,\n",
       "          2231,  3559,  1954,  2231,  3134,  2996,  3559,  1954,  2231,  3134,\n",
       "          5774,  2996, 29228, 18248, 50256]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longets_text = dataset['text'][np.argmax(list(map(len, dataset['text'])))]\n",
    "print(longets_text)\n",
    "tokenizer(longets_text, return_tensors='pt', padding='max_length', max_length=185, truncation=True)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02778ee6-1ff7-4fc9-be85-67c3b4895b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1435 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#text_fixed = tokenizer(dataset['text'], return_tensors='pt', padding='max_length', max_length=128, truncation=True)['input_ids']\n",
    "text_fixed = tokenizer(dataset['text'], return_tensors='pt', padding='max_length', max_length=64, truncation=True)['input_ids']\n",
    "variable_text = tokenizer(dataset['text'])['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99359e1c-dbf9-4d56-858e-18c7291b5efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = list()\n",
    "for c in emotions:\n",
    "    y.append(dataset[c])\n",
    "y = np.array(y).T\n",
    "y[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6533700d-ac9f-4753-a8b6-4a542116f222",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fixed, X_test_fixed, y_train, y_test = sklearn.model_selection.train_test_split(text_fixed, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68c0ff65-27ec-4985-b73d-a072584874ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147857"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0462e916-4b28-4e73-bfb6-3781c1f553ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_var, X_test_var, y_train, y_test = sklearn.model_selection.train_test_split(variable_text, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd4868c1-cc9a-4828-8e52-b552d8159eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147857"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f392f46-1a6b-4d8c-9d9f-07d9061e4dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2898,  1077, 21752,  ..., 50256, 50256, 50256],\n",
       "         [ 1544,  3073,  1107,  ..., 50256, 50256, 50256],\n",
       "         [   40,  1101,   994,  ..., 50256, 50256, 50256],\n",
       "         ...,\n",
       "         [   40,   716,  1719,  ..., 50256, 50256, 50256],\n",
       "         [10049,  9368,   517,  ..., 50256, 50256, 50256],\n",
       "         [ 1544,   373,  8011,  ..., 50256, 50256, 50256]]),\n",
       " tensor([[0, 0, 0,  ..., 0, 0, 1],\n",
       "         [0, 0, 0,  ..., 0, 0, 1],\n",
       "         [0, 0, 0,  ..., 0, 0, 1],\n",
       "         ...,\n",
       "         [0, 1, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 1]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    xx = list(map(torch.LongTensor, xx))\n",
    "    xx_pad = torch.nn.utils.rnn.pad_sequence(xx, batch_first=True, padding_value=tokenizer.eos_token_id)\n",
    "    yy = torch.LongTensor(np.array(yy)) #torch.Tensor(list) very slow\n",
    "    return xx_pad, yy\n",
    "\n",
    "class Dataset_multilabel(Dataset):\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "        \n",
    "BATCH_SIZE = 50\n",
    "\n",
    "training_data_fixed = Dataset_multilabel(X_train_fixed, y_train)\n",
    "train_dataloader_fixed = DataLoader(training_data_fixed, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_data_fixed = Dataset_multilabel(X_test_fixed, y_test)\n",
    "test_dataloader_fixed = DataLoader(test_data_fixed, batch_size=BATCH_SIZE)\n",
    "\n",
    "training_data_var = Dataset_multilabel(X_train_var, y_train)\n",
    "train_dataloader_var = DataLoader(training_data_var, batch_size=100, shuffle=True, collate_fn=pad_collate)# 500 не влезал, потому что там может быть много токенов\n",
    "test_data_var = Dataset_multilabel(X_test_var, y_test)\n",
    "test_dataloader_var = DataLoader(test_data_var, batch_size=100)\n",
    "\n",
    "next(iter(train_dataloader_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b628be8-1f04-4212-9392-dee3e207f532",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066591d4-60c1-4918-85e7-fafe734d1804",
   "metadata": {},
   "source": [
    "Implement your model. The model should have the RNN architecture (with LSTM or GRU cells), support stacking and bidirectional feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e54e3784-2f24-42a2-a8a0-164378480a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(prefix: str | None = None, suffix: str | None = None, separator: str = '_') -> str | None:\n",
    "    return prefix and prefix + separator + suffix or suffix or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90a144e0-b2f7-41a9-8f4f-4a84f40d6077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, units: int, n_tokens: int, n_labels: int, n_stacks: int, bidirectional: bool, name: str, cell_type):\n",
    "        super(Model, self).__init__()\n",
    "        self.name = name\n",
    "        self.units = units\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        #torch.nn.GRU(input_size, hidden_size, num_layers=1, bias=True, batch_first=False, dropout=0.0, bidirectional=False, device=None, dtype=None)\n",
    "        #torch.nn.LSTM(input_size, hidden_size, num_layers=1, bias=True, batch_first=False, dropout=0.0, bidirectional=False, proj_size=0, device=None, dtype=None)\n",
    "        \n",
    "        self.embeddings = torch.nn.Embedding(n_tokens, units)\n",
    "        self.rnn = cell_type(units, units*2, num_layers=n_stacks, bidirectional=bidirectional)\n",
    "        self.FC = torch.nn.Linear(units*2*2 if bidirectional else units*2, n_labels)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embeds = self.embeddings(text)\n",
    "        \n",
    "        # костыль, если у меня без батчей (var_text)\n",
    "        # надо придумать как сюда батчки пихнуть (var_text)\n",
    "        embeds = embeds.permute(1, 0, 2) if len(embeds.shape) == 3 else embeds\n",
    "        out, _ = self.rnn(embeds)\n",
    "        out = out[-1]\n",
    "\n",
    "        out = self.FC(out)\n",
    "        return out\n",
    "\n",
    "def get_model(\n",
    "    units: int,\n",
    "    n_tokens: int,\n",
    "    n_labels: int,\n",
    "    n_stacks: int = 1,\n",
    "    bidirectional: bool = False,\n",
    "    name: str | None = None,\n",
    "    cell_type: type[torch.nn.modules] = torch.nn.LSTM\n",
    ") -> torch.nn.Module:\n",
    "    '''Creates a model with RNN architecture for sequence multilabel classification.\n",
    "\n",
    "    Arguments:\n",
    "        units: dimensionality of RNN cells OR units: Positive integer, dimensionality of the output space\n",
    "        n_tokens: number of tokens in the tokenizer dictionary\n",
    "        n_labels: number of labels to be predicted\n",
    "        n_stacks: number of RNN cells in the stack (1 -- no stacking)\n",
    "        bidirectional: whether or not the model is bidirectional\n",
    "        name: the model name\n",
    "        cell_type: type of a cell to use, either keras.layers.LSTMCell or keras.layers.GRUCell\n",
    "\n",
    "    Returns:\n",
    "        The model'''\n",
    "    return Model(units, n_tokens, n_labels, n_stacks, bidirectional, name, cell_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1d16ca7-0bcd-4213-a3cd-1b3d35662079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0639e-02,  1.2569e-02,  8.6522e-05, -4.1426e-02,  1.0565e-02,\n",
       "         -2.7325e-02,  3.5617e-02, -2.6026e-02, -4.4977e-03, -4.3532e-02,\n",
       "         -5.0141e-02,  3.6737e-02, -3.4961e-02, -5.2866e-02,  1.2826e-02,\n",
       "          3.1861e-02,  4.7196e-03,  3.8590e-02, -2.4468e-02,  4.0325e-02,\n",
       "          6.8094e-03,  3.3694e-02, -2.0026e-02, -3.9810e-02, -1.9765e-02,\n",
       "         -4.6007e-03,  2.8957e-02,  2.2390e-02],\n",
       "        [-2.0639e-02,  1.2569e-02,  8.6525e-05, -4.1426e-02,  1.0565e-02,\n",
       "         -2.7325e-02,  3.5617e-02, -2.6026e-02, -4.4977e-03, -4.3532e-02,\n",
       "         -5.0141e-02,  3.6737e-02, -3.4961e-02, -5.2866e-02,  1.2826e-02,\n",
       "          3.1861e-02,  4.7196e-03,  3.8590e-02, -2.4468e-02,  4.0325e-02,\n",
       "          6.8094e-03,  3.3694e-02, -2.0026e-02, -3.9810e-02, -1.9765e-02,\n",
       "         -4.6007e-03,  2.8957e-02,  2.2390e-02],\n",
       "        [-2.0639e-02,  1.2569e-02,  8.6517e-05, -4.1426e-02,  1.0565e-02,\n",
       "         -2.7325e-02,  3.5617e-02, -2.6026e-02, -4.4977e-03, -4.3532e-02,\n",
       "         -5.0141e-02,  3.6737e-02, -3.4961e-02, -5.2866e-02,  1.2826e-02,\n",
       "          3.1861e-02,  4.7196e-03,  3.8590e-02, -2.4468e-02,  4.0325e-02,\n",
       "          6.8094e-03,  3.3694e-02, -2.0026e-02, -3.9810e-02, -1.9765e-02,\n",
       "         -4.6007e-03,  2.8957e-02,  2.2390e-02]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(units=128, \n",
    "                  n_tokens=len(tokenizer.vocab), \n",
    "                  n_labels=len(emotions), \n",
    "                  n_stacks=100, \n",
    "                  bidirectional=True, \n",
    "                  name='LSTM', \n",
    "                  cell_type=torch.nn.LSTM)\n",
    "#model(torch.Tensor(variable_text[0]).to(int))\n",
    "model(text_fixed[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1790d12f-d3f9-477e-bbfc-067baadf33c6",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42397b6a-e849-4179-a359-647e902300f4",
   "metadata": {},
   "source": [
    "Train several models on the two dataset variants. Use either of the cell types (LSTM or GRU)\n",
    "* Simple RNN (no stacking, one direction)\n",
    "* Stacked RNN (stacking, one direction)\n",
    "* Bidirectional RNN (no stacking, bidirectional)\n",
    "* Stacked Bidirectional RNN (stacking, bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1382d9b-3da7-4a5d-a272-7ea53efc5e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#units, name, bidirectional, n_stacks, cell_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07bf0549-2961-432a-a457-9d79bd22f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = ['Simple RNN', 'Stacked RNN', 'Bidirectional RNN', 'Stacked Bidirectional RNN']\n",
    "architecture = ['LSTM ', 'GRU ']\n",
    "all_names = [arch + config for arch in architecture for config in configs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a71e161d-5e01-4d4e-8027-4a5a8838c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(verbose = False):\n",
    "    super_zip = zip([64] * 8,                                  #units\n",
    "                    all_names,                                 #name\n",
    "                    ([False] * 2 + [True] * 2) * 2,            #bidirectional\n",
    "                    [1, 3] * 4,                                #n_stacks\n",
    "                    [torch.nn.LSTM] * 4 + [torch.nn.GRU] * 4,) #cell_type\n",
    "\n",
    "    models = [\n",
    "        get_model(\n",
    "            units=units,\n",
    "            n_tokens=len(tokenizer.get_vocab()),\n",
    "            n_labels=len(emotions),\n",
    "            name=name,\n",
    "            bidirectional=bidirectional,\n",
    "            n_stacks=n_stacks,\n",
    "            cell_type=cell_type\n",
    "        )\n",
    "        for units, name, bidirectional, n_stacks, cell_type in super_zip\n",
    "    ]\n",
    "    if verbose:\n",
    "        print(*[(model.name, model) for model in models], sep='\\n')\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92142b18-0255-416d-812b-b2cce689196a",
   "metadata": {},
   "source": [
    "Which loss should be used to multilabel classification? Which metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be315f1d-1b09-4a6e-a301-c2e9ac0d3bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# about hamming score https://wiki.cloudfactory.com/docs/mp-wiki/metrics/hamming-score\n",
    "# about metrics 4 multilabel https://mmuratarat.github.io/2020-01-25/multilabel_classification_metrics\n",
    "def Hamming_score(y_true, y_pred, tr=0.5, use_sigmoid=True):\n",
    "    \n",
    "    if use_sigmoid:\n",
    "        y_pred = 1 / (1 + np.exp(-y_pred))\n",
    "        y_pred = (y_pred > tr).astype(int)\n",
    "\n",
    "    # temp = 0\n",
    "    # for i in range(y_true.shape[0]):\n",
    "    #     temp += sum(np.logical_and(y_true[i], y_pred[i])) / sum(np.logical_or(y_true[i], y_pred[i]))\n",
    "    #return temp / y_true.shape[0]\n",
    "    return ((y_true & y_pred).sum(axis=-1) / (y_true | y_pred).sum(axis=-1)).mean()\n",
    "test_true = np.array([[0, 1, 1], \n",
    "                      [0, 1, 1]])\n",
    "\n",
    "print(Hamming_score(test_true, np.array([[0, 1, 0], \n",
    "                                         [0, 1, 0]])), \n",
    "      Hamming_score(test_true, np.array([[1, 0, 0], \n",
    "                                         [1, 0, 0]])), \n",
    "      Hamming_score(test_true, np.array([[0, 1, 1], \n",
    "                                         [0, 1, 1]])))\n",
    "\n",
    "# пример для 1 объекта\n",
    "# hamping = 0\n",
    "# y_true, y_pred = np.array([0, 1, 1]), np.array([0, 1, 1])\n",
    "# hamping = sum(np.logical_and(y_true, y_pred)) / sum(np.logical_or(y_true, y_pred))\n",
    "# hamping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c416bf78-a954-4837-968c-0ff4365d3fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, dataloader, batch_size):\n",
    "    \n",
    "    size = len(dataloader)\n",
    "    model.train()\n",
    "\n",
    "    #percent_of_batch = len(training_data) // batch_size / 2\n",
    "    running_loss = 0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to('cuda'), y.to('cuda')\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y.float())\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # if batch % percent_of_batch == 0:\n",
    "        #     loss = loss.item()\n",
    "        #     print(f\"{model.name} loss: {loss:>7f}\")\n",
    "    return running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b63491c3-5aa6-4f89-a37d-79a33005ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loss_fn, dataloader, metric, tr):\n",
    "    \n",
    "    model.eval()\n",
    "    running_metric = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to('cuda'), y.to('cuda')\n",
    "            y_pred = 1 / (1 + torch.exp(-model(X))) #сигмойдификация\n",
    "            #y_pred = model(X)\n",
    "            #y_pred = ((1 / (1 + np.exp(-y_pred))) > tr).astype(int)\n",
    "            # running_metric += (y.cpu().numpy() == y_pred).mean()\n",
    "            running_metric += metric(y_pred, y)\n",
    "    return running_metric / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25b2824c-1fa5-42a1-bfa8-c17f861ca49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "####optimizer = torch.optim.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2aeec575-bc0c-44dc-a760-b1827a1067e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mMultilabelAUROC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0maverage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'macro'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weighted'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'macro'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mthresholds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvalidate_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Compute Area Under the Receiver Operating Characteristic Curve (`ROC AUC`_) for multilabel tasks.\n",
       "\n",
       "The AUROC score summarizes the ROC curve into an single number that describes the performance of a model for\n",
       "multiple thresholds at the same time. Notably, an AUROC score of 1 is a perfect score and an AUROC score of 0.5\n",
       "corresponds to random guessing.\n",
       "\n",
       "As input to ``forward`` and ``update`` the metric accepts the following input:\n",
       "\n",
       "- ``preds`` (:class:`~torch.Tensor`): A float tensor of shape ``(N, C, ...)`` containing probabilities or logits\n",
       "  for each observation. If preds has values outside [0,1] range we consider the input to be logits and will auto\n",
       "  apply sigmoid per element.\n",
       "- ``target`` (:class:`~torch.Tensor`): An int tensor of shape ``(N, C, ...)`` containing ground truth labels, and\n",
       "  therefore only contain {0,1} values (except if `ignore_index` is specified).\n",
       "\n",
       "As output to ``forward`` and ``compute`` the metric returns the following output:\n",
       "\n",
       "- ``ml_auroc`` (:class:`~torch.Tensor`): If `average=None|\"none\"` then a 1d tensor of shape (n_classes, ) will\n",
       "  be returned with auroc score per class. If `average=\"micro|macro\"|\"weighted\"` then a single scalar is returned.\n",
       "\n",
       "Additional dimension ``...`` will be flattened into the batch dimension.\n",
       "\n",
       "The implementation both supports calculating the metric in a non-binned but accurate version and a binned version\n",
       "that is less accurate but more memory efficient. Setting the `thresholds` argument to `None` will activate the\n",
       "non-binned  version that uses memory of size :math:`\\mathcal{O}(n_{samples})` whereas setting the `thresholds`\n",
       "argument to either an integer, list or a 1d tensor will use a binned version that uses memory of\n",
       "size :math:`\\mathcal{O}(n_{thresholds} \\times n_{labels})` (constant memory).\n",
       "\n",
       "Args:\n",
       "    num_labels: Integer specifying the number of labels\n",
       "    average:\n",
       "        Defines the reduction that is applied over labels. Should be one of the following:\n",
       "\n",
       "        - ``micro``: Sum score over all labels\n",
       "        - ``macro``: Calculate score for each label and average them\n",
       "        - ``weighted``: calculates score for each label and computes weighted average using their support\n",
       "        - ``\"none\"`` or ``None``: calculates score for each label and applies no reduction\n",
       "    thresholds:\n",
       "        Can be one of:\n",
       "\n",
       "        - If set to `None`, will use a non-binned approach where thresholds are dynamically calculated from\n",
       "          all the data. Most accurate but also most memory consuming approach.\n",
       "        - If set to an `int` (larger than 1), will use that number of thresholds linearly spaced from\n",
       "          0 to 1 as bins for the calculation.\n",
       "        - If set to an `list` of floats, will use the indicated thresholds in the list as bins for the calculation\n",
       "        - If set to an 1d `tensor` of floats, will use the indicated thresholds in the tensor as\n",
       "          bins for the calculation.\n",
       "\n",
       "    validate_args: bool indicating if input arguments and tensors should be validated for correctness.\n",
       "        Set to ``False`` for faster computations.\n",
       "    kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.\n",
       "\n",
       "Example:\n",
       "    >>> from torch import tensor\n",
       "    >>> from torchmetrics.classification import MultilabelAUROC\n",
       "    >>> preds = tensor([[0.75, 0.05, 0.35],\n",
       "    ...                       [0.45, 0.75, 0.05],\n",
       "    ...                       [0.05, 0.55, 0.75],\n",
       "    ...                       [0.05, 0.65, 0.05]])\n",
       "    >>> target = tensor([[1, 0, 1],\n",
       "    ...                        [0, 0, 0],\n",
       "    ...                        [0, 1, 1],\n",
       "    ...                        [1, 1, 1]])\n",
       "    >>> ml_auroc = MultilabelAUROC(num_labels=3, average=\"macro\", thresholds=None)\n",
       "    >>> ml_auroc(preds, target)\n",
       "    tensor(0.6528)\n",
       "    >>> ml_auroc = MultilabelAUROC(num_labels=3, average=None, thresholds=None)\n",
       "    >>> ml_auroc(preds, target)\n",
       "    tensor([0.6250, 0.5000, 0.8333])\n",
       "    >>> ml_auroc = MultilabelAUROC(num_labels=3, average=\"macro\", thresholds=5)\n",
       "    >>> ml_auroc(preds, target)\n",
       "    tensor(0.6528)\n",
       "    >>> ml_auroc = MultilabelAUROC(num_labels=3, average=None, thresholds=5)\n",
       "    >>> ml_auroc(preds, target)\n",
       "    tensor([0.6250, 0.5000, 0.8333])\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/Рабочий стол/NN-NLP/venv/lib/python3.12/site-packages/torchmetrics/classification/auroc.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MultilabelAUROC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1652d9f-816d-4b36-9bf7-21d38e764e2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "LSTM Simple RNN loss: 0.15969777363147825\n",
      "MultilabelAUROC(): 36.32%\n",
      "----------------------------------------\n",
      "Epoch 1:\n",
      "LSTM Simple RNN loss: 0.15796892569123933\n",
      "MultilabelAUROC(): 36.29%\n",
      "----------------------------------------\n",
      "Epoch 2:\n",
      "LSTM Simple RNN loss: 0.15796902894903228\n",
      "MultilabelAUROC(): 36.29%\n",
      "----------------------------------------\n",
      "Epoch 3:\n",
      "LSTM Simple RNN loss: 0.15796329425125205\n",
      "MultilabelAUROC(): 36.31%\n",
      "----------------------------------------\n",
      "Epoch 4:\n",
      "LSTM Simple RNN loss: 0.15799336093575508\n",
      "MultilabelAUROC(): 36.36%\n",
      "----------------------------------------\n",
      "Epoch 5:\n",
      "LSTM Simple RNN loss: 0.15793089786742168\n",
      "MultilabelAUROC(): 36.35%\n",
      "----------------------------------------\n",
      "Epoch 6:\n",
      "LSTM Simple RNN loss: 0.1579217937944345\n",
      "MultilabelAUROC(): 36.31%\n",
      "----------------------------------------\n",
      "Epoch 7:\n",
      "LSTM Simple RNN loss: 0.15795600020337378\n",
      "MultilabelAUROC(): 36.31%\n",
      "----------------------------------------\n",
      "Epoch 8:\n",
      "LSTM Simple RNN loss: 0.15794140234226872\n",
      "MultilabelAUROC(): 36.43%\n",
      "----------------------------------------\n",
      "Epoch 9:\n",
      "LSTM Simple RNN loss: 0.1578926976475464\n",
      "MultilabelAUROC(): 36.31%\n",
      "----------------------------------------\n",
      "========================================\n",
      "Epoch 0:\n",
      "LSTM Stacked RNN loss: 0.1595607766852619\n",
      "MultilabelAUROC(): 36.30%\n",
      "----------------------------------------\n",
      "Epoch 1:\n",
      "LSTM Stacked RNN loss: 0.1580415417372858\n",
      "MultilabelAUROC(): 36.30%\n",
      "----------------------------------------\n",
      "Epoch 2:\n",
      "LSTM Stacked RNN loss: 0.15802818774532193\n",
      "MultilabelAUROC(): 36.30%\n",
      "----------------------------------------\n",
      "Epoch 3:\n",
      "LSTM Stacked RNN loss: 0.15806169956015767\n",
      "MultilabelAUROC(): 36.31%\n",
      "----------------------------------------\n",
      "Epoch 4:\n",
      "LSTM Stacked RNN loss: 0.15802963709230758\n",
      "MultilabelAUROC(): 36.31%\n",
      "----------------------------------------\n",
      "Epoch 5:\n",
      "LSTM Stacked RNN loss: 0.15803446832532977\n",
      "MultilabelAUROC(): 36.31%\n",
      "----------------------------------------\n",
      "Epoch 6:\n",
      "LSTM Stacked RNN loss: 0.15802149943619503\n",
      "MultilabelAUROC(): 36.31%\n",
      "----------------------------------------\n",
      "Epoch 7:\n",
      "LSTM Stacked RNN loss: 0.15804192686782165\n",
      "MultilabelAUROC(): 36.31%\n",
      "----------------------------------------\n",
      "Epoch 8:\n",
      "LSTM Stacked RNN loss: 0.15806916140478314\n",
      "MultilabelAUROC(): 36.31%\n",
      "----------------------------------------\n",
      "Epoch 9:\n",
      "LSTM Stacked RNN loss: 0.158046742403745\n",
      "MultilabelAUROC(): 36.31%\n",
      "----------------------------------------\n",
      "========================================\n",
      "Epoch 0:\n",
      "LSTM Bidirectional RNN loss: 0.15953116467760334\n",
      "MultilabelAUROC(): 36.29%\n",
      "----------------------------------------\n",
      "Epoch 1:\n",
      "LSTM Bidirectional RNN loss: 0.15779238272309706\n",
      "MultilabelAUROC(): 36.34%\n",
      "----------------------------------------\n",
      "Epoch 2:\n",
      "LSTM Bidirectional RNN loss: 0.15774589471715458\n",
      "MultilabelAUROC(): 36.33%\n",
      "----------------------------------------\n",
      "Epoch 3:\n",
      "LSTM Bidirectional RNN loss: 0.15768631311426942\n",
      "MultilabelAUROC(): 36.32%\n",
      "----------------------------------------\n",
      "Epoch 4:\n",
      "LSTM Bidirectional RNN loss: 0.15768221994714612\n",
      "MultilabelAUROC(): 36.34%\n",
      "----------------------------------------\n",
      "Epoch 5:\n",
      "LSTM Bidirectional RNN loss: 0.15766052619136614\n",
      "MultilabelAUROC(): 36.32%\n",
      "----------------------------------------\n",
      "Epoch 6:\n",
      "LSTM Bidirectional RNN loss: 0.15763304200202247\n",
      "MultilabelAUROC(): 36.31%\n",
      "----------------------------------------\n",
      "Epoch 7:\n",
      "LSTM Bidirectional RNN loss: 0.157596370201505\n",
      "MultilabelAUROC(): 36.29%\n",
      "----------------------------------------\n",
      "Epoch 8:\n",
      "LSTM Bidirectional RNN loss: 0.15762822165017276\n",
      "MultilabelAUROC(): 36.31%\n",
      "----------------------------------------\n",
      "Epoch 9:\n",
      "LSTM Bidirectional RNN loss: 0.15756881856400473\n",
      "MultilabelAUROC(): 36.32%\n",
      "----------------------------------------\n",
      "========================================\n",
      "Epoch 0:\n",
      "LSTM Stacked Bidirectional RNN loss: 0.15960384671090824\n",
      "MultilabelAUROC(): 36.30%\n",
      "----------------------------------------\n",
      "Epoch 1:\n",
      "LSTM Stacked Bidirectional RNN loss: 0.15806318932364646\n",
      "MultilabelAUROC(): 36.30%\n",
      "----------------------------------------\n",
      "Epoch 2:\n",
      "LSTM Stacked Bidirectional RNN loss: 0.15801170792750527\n",
      "MultilabelAUROC(): 36.39%\n",
      "----------------------------------------\n",
      "Epoch 3:\n",
      "LSTM Stacked Bidirectional RNN loss: 0.1580072497549776\n",
      "MultilabelAUROC(): 36.32%\n",
      "----------------------------------------\n",
      "Epoch 4:\n",
      "LSTM Stacked Bidirectional RNN loss: 0.15797149170091862\n",
      "MultilabelAUROC(): 36.31%\n",
      "----------------------------------------\n",
      "Epoch 5:\n",
      "LSTM Stacked Bidirectional RNN loss: 0.15798650487018165\n",
      "MultilabelAUROC(): 36.25%\n",
      "----------------------------------------\n",
      "Epoch 6:\n",
      "LSTM Stacked Bidirectional RNN loss: 0.15796061501440928\n",
      "MultilabelAUROC(): 36.30%\n",
      "----------------------------------------\n",
      "Epoch 7:\n",
      "LSTM Stacked Bidirectional RNN loss: 0.1579313822508745\n",
      "MultilabelAUROC(): 36.31%\n",
      "----------------------------------------\n",
      "Epoch 8:\n",
      "LSTM Stacked Bidirectional RNN loss: 0.15793434722932148\n",
      "MultilabelAUROC(): 36.29%\n",
      "----------------------------------------\n",
      "Epoch 9:\n",
      "LSTM Stacked Bidirectional RNN loss: 0.1579532248469887\n",
      "MultilabelAUROC(): 36.31%\n",
      "----------------------------------------\n",
      "========================================\n",
      "Epoch 0:\n",
      "GRU Simple RNN loss: 0.1596727073293348\n",
      "MultilabelAUROC(): 36.29%\n",
      "----------------------------------------\n",
      "Epoch 1:\n",
      "GRU Simple RNN loss: 0.157967544862896\n",
      "MultilabelAUROC(): 36.36%\n",
      "----------------------------------------\n",
      "Epoch 2:\n",
      "GRU Simple RNN loss: 0.15781211400400794\n",
      "MultilabelAUROC(): 36.28%\n",
      "----------------------------------------\n",
      "Epoch 3:\n",
      "GRU Simple RNN loss: 0.1571485836731372\n",
      "MultilabelAUROC(): 36.31%\n",
      "----------------------------------------\n",
      "Epoch 4:\n",
      "GRU Simple RNN loss: 0.15596381680406454\n",
      "MultilabelAUROC(): 36.29%\n",
      "----------------------------------------\n",
      "Epoch 5:\n",
      "GRU Simple RNN loss: 0.15436460245514014\n",
      "MultilabelAUROC(): 36.35%\n",
      "----------------------------------------\n",
      "Epoch 6:\n",
      "GRU Simple RNN loss: 0.15250068104512954\n",
      "MultilabelAUROC(): 36.42%\n",
      "----------------------------------------\n",
      "Epoch 7:\n",
      "GRU Simple RNN loss: 0.15060576646663676\n",
      "MultilabelAUROC(): 36.34%\n",
      "----------------------------------------\n",
      "Epoch 8:\n",
      "GRU Simple RNN loss: 0.1486914461255678\n",
      "MultilabelAUROC(): 36.37%\n",
      "----------------------------------------\n",
      "Epoch 9:\n",
      "GRU Simple RNN loss: 0.14677129543819\n",
      "MultilabelAUROC(): 36.42%\n",
      "----------------------------------------\n",
      "========================================\n",
      "Epoch 0:\n",
      "GRU Stacked RNN loss: 0.1595046911804236\n",
      "MultilabelAUROC(): 36.36%\n",
      "----------------------------------------\n",
      "Epoch 1:\n",
      "GRU Stacked RNN loss: 0.15807376842520138\n",
      "MultilabelAUROC(): 36.35%\n",
      "----------------------------------------\n",
      "Epoch 2:\n",
      "GRU Stacked RNN loss: 0.15804373668387098\n",
      "MultilabelAUROC(): 36.33%\n",
      "----------------------------------------\n",
      "Epoch 3:\n",
      "GRU Stacked RNN loss: 0.1580483738667986\n",
      "MultilabelAUROC(): 36.26%\n",
      "----------------------------------------\n",
      "Epoch 4:\n",
      "GRU Stacked RNN loss: 0.1579430093554285\n",
      "MultilabelAUROC(): 36.41%\n",
      "----------------------------------------\n",
      "Epoch 5:\n",
      "GRU Stacked RNN loss: 0.1578592622502782\n",
      "MultilabelAUROC(): 36.25%\n",
      "----------------------------------------\n",
      "Epoch 6:\n",
      "GRU Stacked RNN loss: 0.15771687196276735\n",
      "MultilabelAUROC(): 36.38%\n",
      "----------------------------------------\n",
      "Epoch 7:\n",
      "GRU Stacked RNN loss: 0.15749986901466229\n",
      "MultilabelAUROC(): 36.41%\n",
      "----------------------------------------\n",
      "Epoch 8:\n",
      "GRU Stacked RNN loss: 0.15716850750042508\n",
      "MultilabelAUROC(): 36.36%\n",
      "----------------------------------------\n",
      "Epoch 9:\n",
      "GRU Stacked RNN loss: 0.1567885877994003\n",
      "MultilabelAUROC(): 36.28%\n",
      "----------------------------------------\n",
      "========================================\n",
      "Epoch 0:\n",
      "GRU Bidirectional RNN loss: 0.15925147684393015\n",
      "MultilabelAUROC(): 36.22%\n",
      "----------------------------------------\n",
      "Epoch 1:\n",
      "GRU Bidirectional RNN loss: 0.15796528361168483\n",
      "MultilabelAUROC(): 36.41%\n",
      "----------------------------------------\n",
      "Epoch 2:\n",
      "GRU Bidirectional RNN loss: 0.15788524095132755\n",
      "MultilabelAUROC(): 36.25%\n",
      "----------------------------------------\n",
      "Epoch 3:\n",
      "GRU Bidirectional RNN loss: 0.1577360820657422\n",
      "MultilabelAUROC(): 36.28%\n",
      "----------------------------------------\n",
      "Epoch 4:\n",
      "GRU Bidirectional RNN loss: 0.15736137667546166\n",
      "MultilabelAUROC(): 36.28%\n",
      "----------------------------------------\n",
      "Epoch 5:\n",
      "GRU Bidirectional RNN loss: 0.15654797786047042\n",
      "MultilabelAUROC(): 36.24%\n",
      "----------------------------------------\n",
      "Epoch 6:\n",
      "GRU Bidirectional RNN loss: 0.15539435309844699\n",
      "MultilabelAUROC(): 36.28%\n",
      "----------------------------------------\n",
      "Epoch 7:\n",
      "GRU Bidirectional RNN loss: 0.15393035449818068\n",
      "MultilabelAUROC(): 36.33%\n",
      "----------------------------------------\n",
      "Epoch 8:\n",
      "GRU Bidirectional RNN loss: 0.15225401503097535\n",
      "MultilabelAUROC(): 36.31%\n",
      "----------------------------------------\n",
      "Epoch 9:\n",
      "GRU Bidirectional RNN loss: 0.15053961208899977\n",
      "MultilabelAUROC(): 36.31%\n",
      "----------------------------------------\n",
      "========================================\n",
      "Epoch 0:\n",
      "GRU Stacked Bidirectional RNN loss: 0.15915861046993304\n",
      "MultilabelAUROC(): 36.25%\n",
      "----------------------------------------\n",
      "Epoch 1:\n",
      "GRU Stacked Bidirectional RNN loss: 0.1580793771079217\n",
      "MultilabelAUROC(): 36.28%\n",
      "----------------------------------------\n",
      "Epoch 2:\n",
      "GRU Stacked Bidirectional RNN loss: 0.1579991327126418\n",
      "MultilabelAUROC(): 36.27%\n",
      "----------------------------------------\n",
      "Epoch 3:\n",
      "GRU Stacked Bidirectional RNN loss: 0.15785536792902546\n",
      "MultilabelAUROC(): 36.20%\n",
      "----------------------------------------\n",
      "Epoch 4:\n",
      "GRU Stacked Bidirectional RNN loss: 0.15785065198101814\n",
      "MultilabelAUROC(): 36.35%\n",
      "----------------------------------------\n",
      "Epoch 5:\n",
      "GRU Stacked Bidirectional RNN loss: 0.15784222603932355\n",
      "MultilabelAUROC(): 36.34%\n",
      "----------------------------------------\n",
      "Epoch 6:\n",
      "GRU Stacked Bidirectional RNN loss: 0.1577991246900057\n",
      "MultilabelAUROC(): 36.27%\n",
      "----------------------------------------\n",
      "Epoch 7:\n",
      "GRU Stacked Bidirectional RNN loss: 0.1578123161318618\n",
      "MultilabelAUROC(): 36.24%\n",
      "----------------------------------------\n",
      "Epoch 8:\n",
      "GRU Stacked Bidirectional RNN loss: 0.15776928003431576\n",
      "MultilabelAUROC(): 36.40%\n",
      "----------------------------------------\n",
      "Epoch 9:\n",
      "GRU Stacked Bidirectional RNN loss: 0.15764907522327276\n",
      "MultilabelAUROC(): 36.52%\n",
      "----------------------------------------\n",
      "========================================\n",
      "CPU times: user 56min 31s, sys: 15.1 s, total: 56min 46s\n",
      "Wall time: 56min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#fixed\n",
    "EPOCHS = 10\n",
    "models = get_models()\n",
    "\n",
    "for model in models:\n",
    "    model.to('cuda')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "    \n",
    "    for i in range(EPOCHS):\n",
    "        \n",
    "        train_loss = train(model, loss_fn, optimizer, train_dataloader_fixed, BATCH_SIZE)\n",
    "        metric = MultilabelAUROC(num_labels=len(emotions), average='macro').to('cuda')\n",
    "        #metric = MultilabelAccuracy(criteria=\"hamming\")\n",
    "        train_metric = test(model, loss_fn, test_dataloader_fixed, metric, 0.4)\n",
    "        scheduler.step(train_loss) #надо лосс на валидации сделать\n",
    "        #if not i % 2:\n",
    "        print(f'Epoch {i}:')\n",
    "        print(f'{model.name} loss: {train_loss}')\n",
    "        print(f'{metric}: {train_metric * 100 :.2f}%')\n",
    "        print('-'*40)\n",
    "    print('='*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a258e251-9b44-4a65-b50e-a89e33e3fd2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "LSTM Simple RNN loss: 0.1427702632675387\n",
      "MultilabelAUROC(): 64.56%\n",
      "----------------------------------------\n",
      "Epoch 1:\n",
      "LSTM Simple RNN loss: 0.1331341359424301\n",
      "MultilabelAUROC(): 69.43%\n",
      "----------------------------------------\n",
      "Epoch 2:\n",
      "LSTM Simple RNN loss: 0.1258689112976405\n",
      "MultilabelAUROC(): 72.34%\n",
      "----------------------------------------\n",
      "Epoch 3:\n",
      "LSTM Simple RNN loss: 0.12040394105828235\n",
      "MultilabelAUROC(): 74.37%\n",
      "----------------------------------------\n",
      "Epoch 4:\n",
      "LSTM Simple RNN loss: 0.11616550598537866\n",
      "MultilabelAUROC(): 75.72%\n",
      "----------------------------------------\n",
      "Epoch 5:\n",
      "LSTM Simple RNN loss: 0.11264198714712328\n",
      "MultilabelAUROC(): 76.77%\n",
      "----------------------------------------\n",
      "Epoch 6:\n",
      "LSTM Simple RNN loss: 0.10957778270902305\n",
      "MultilabelAUROC(): 77.62%\n",
      "----------------------------------------\n",
      "Epoch 7:\n",
      "LSTM Simple RNN loss: 0.10681055761757857\n",
      "MultilabelAUROC(): 78.18%\n",
      "----------------------------------------\n",
      "Epoch 8:\n",
      "LSTM Simple RNN loss: 0.10426565236018427\n",
      "MultilabelAUROC(): 78.84%\n",
      "----------------------------------------\n",
      "Epoch 9:\n",
      "LSTM Simple RNN loss: 0.10207152203420597\n",
      "MultilabelAUROC(): 79.46%\n",
      "----------------------------------------\n",
      "========================================\n",
      "Epoch 0:\n",
      "LSTM Stacked RNN loss: 0.1578062118671487\n",
      "MultilabelAUROC(): 43.45%\n",
      "----------------------------------------\n",
      "Epoch 1:\n",
      "LSTM Stacked RNN loss: 0.15772488841218993\n",
      "MultilabelAUROC(): 43.68%\n",
      "----------------------------------------\n",
      "Epoch 2:\n",
      "LSTM Stacked RNN loss: 0.15776587432304534\n",
      "MultilabelAUROC(): 43.53%\n",
      "----------------------------------------\n",
      "Epoch 3:\n",
      "LSTM Stacked RNN loss: 0.15776974404075808\n",
      "MultilabelAUROC(): 43.62%\n",
      "----------------------------------------\n",
      "Epoch 4:\n",
      "LSTM Stacked RNN loss: 0.1577668337603531\n",
      "MultilabelAUROC(): 43.52%\n",
      "----------------------------------------\n",
      "Epoch 5:\n",
      "LSTM Stacked RNN loss: 0.15775597460210283\n",
      "MultilabelAUROC(): 43.52%\n",
      "----------------------------------------\n",
      "Epoch 6:\n",
      "LSTM Stacked RNN loss: 0.15776300737590224\n",
      "MultilabelAUROC(): 43.43%\n",
      "----------------------------------------\n",
      "Epoch 7:\n",
      "LSTM Stacked RNN loss: 0.15774012139876845\n",
      "MultilabelAUROC(): 43.50%\n",
      "----------------------------------------\n",
      "Epoch 8:\n",
      "LSTM Stacked RNN loss: 0.1577775919856535\n",
      "MultilabelAUROC(): 43.27%\n",
      "----------------------------------------\n",
      "Epoch 9:\n",
      "LSTM Stacked RNN loss: 0.15776771806719497\n",
      "MultilabelAUROC(): 43.51%\n",
      "----------------------------------------\n",
      "========================================\n",
      "Epoch 0:\n",
      "LSTM Bidirectional RNN loss: 0.15745637359814196\n",
      "MultilabelAUROC(): 47.20%\n",
      "----------------------------------------\n",
      "Epoch 1:\n",
      "LSTM Bidirectional RNN loss: 0.1544433781188271\n",
      "MultilabelAUROC(): 59.09%\n",
      "----------------------------------------\n",
      "Epoch 2:\n",
      "LSTM Bidirectional RNN loss: 0.1381149636109267\n",
      "MultilabelAUROC(): 67.38%\n",
      "----------------------------------------\n",
      "Epoch 3:\n",
      "LSTM Bidirectional RNN loss: 0.12852555236315066\n",
      "MultilabelAUROC(): 71.40%\n",
      "----------------------------------------\n",
      "Epoch 4:\n",
      "LSTM Bidirectional RNN loss: 0.12220291968150426\n",
      "MultilabelAUROC(): 73.52%\n",
      "----------------------------------------\n",
      "Epoch 5:\n",
      "LSTM Bidirectional RNN loss: 0.1176967874693258\n",
      "MultilabelAUROC(): 75.07%\n",
      "----------------------------------------\n",
      "Epoch 6:\n",
      "LSTM Bidirectional RNN loss: 0.1140694607898059\n",
      "MultilabelAUROC(): 76.10%\n",
      "----------------------------------------\n",
      "Epoch 7:\n",
      "LSTM Bidirectional RNN loss: 0.11106828853054737\n",
      "MultilabelAUROC(): 77.04%\n",
      "----------------------------------------\n",
      "Epoch 8:\n",
      "LSTM Bidirectional RNN loss: 0.10843711750401767\n",
      "MultilabelAUROC(): 77.81%\n",
      "----------------------------------------\n",
      "Epoch 9:\n",
      "LSTM Bidirectional RNN loss: 0.10609807802000265\n",
      "MultilabelAUROC(): 78.36%\n",
      "----------------------------------------\n",
      "========================================\n",
      "Epoch 0:\n",
      "LSTM Stacked Bidirectional RNN loss: 0.15773303467248886\n",
      "MultilabelAUROC(): 45.51%\n",
      "----------------------------------------\n",
      "Epoch 1:\n",
      "LSTM Stacked Bidirectional RNN loss: 0.15368407067009693\n",
      "MultilabelAUROC(): 58.86%\n",
      "----------------------------------------\n",
      "Epoch 2:\n",
      "LSTM Stacked Bidirectional RNN loss: 0.14127361450024437\n",
      "MultilabelAUROC(): 64.74%\n",
      "----------------------------------------\n",
      "Epoch 3:\n",
      "LSTM Stacked Bidirectional RNN loss: 0.13373907151719538\n",
      "MultilabelAUROC(): 68.96%\n",
      "----------------------------------------\n",
      "Epoch 4:\n",
      "LSTM Stacked Bidirectional RNN loss: 0.12691990484592477\n",
      "MultilabelAUROC(): 71.65%\n",
      "----------------------------------------\n",
      "Epoch 5:\n",
      "LSTM Stacked Bidirectional RNN loss: 0.12185879265782477\n",
      "MultilabelAUROC(): 73.62%\n",
      "----------------------------------------\n",
      "Epoch 6:\n",
      "LSTM Stacked Bidirectional RNN loss: 0.11775329807789604\n",
      "MultilabelAUROC(): 75.10%\n",
      "----------------------------------------\n",
      "Epoch 7:\n",
      "LSTM Stacked Bidirectional RNN loss: 0.11426784328365745\n",
      "MultilabelAUROC(): 75.99%\n",
      "----------------------------------------\n",
      "Epoch 8:\n",
      "LSTM Stacked Bidirectional RNN loss: 0.11107113397729969\n",
      "MultilabelAUROC(): 77.10%\n",
      "----------------------------------------\n",
      "Epoch 9:\n",
      "LSTM Stacked Bidirectional RNN loss: 0.10835457214376586\n",
      "MultilabelAUROC(): 77.79%\n",
      "----------------------------------------\n",
      "========================================\n",
      "Epoch 0:\n",
      "GRU Simple RNN loss: 0.15073680791039173\n",
      "MultilabelAUROC(): 63.02%\n",
      "----------------------------------------\n",
      "Epoch 1:\n",
      "GRU Simple RNN loss: 0.135110339380302\n",
      "MultilabelAUROC(): 69.05%\n",
      "----------------------------------------\n",
      "Epoch 2:\n",
      "GRU Simple RNN loss: 0.12645693676970632\n",
      "MultilabelAUROC(): 72.48%\n",
      "----------------------------------------\n",
      "Epoch 3:\n",
      "GRU Simple RNN loss: 0.12021740619156149\n",
      "MultilabelAUROC(): 74.54%\n",
      "----------------------------------------\n",
      "Epoch 4:\n",
      "GRU Simple RNN loss: 0.11543652620305074\n",
      "MultilabelAUROC(): 76.12%\n",
      "----------------------------------------\n",
      "Epoch 5:\n",
      "GRU Simple RNN loss: 0.1114499415444151\n",
      "MultilabelAUROC(): 77.19%\n",
      "----------------------------------------\n",
      "Epoch 6:\n",
      "GRU Simple RNN loss: 0.10812926091419997\n",
      "MultilabelAUROC(): 78.06%\n",
      "----------------------------------------\n",
      "Epoch 7:\n",
      "GRU Simple RNN loss: 0.10521388592915087\n",
      "MultilabelAUROC(): 78.74%\n",
      "----------------------------------------\n",
      "Epoch 8:\n",
      "GRU Simple RNN loss: 0.10264535909713805\n",
      "MultilabelAUROC(): 79.34%\n",
      "----------------------------------------\n",
      "Epoch 9:\n",
      "GRU Simple RNN loss: 0.10038084549384797\n",
      "MultilabelAUROC(): 80.00%\n",
      "----------------------------------------\n",
      "========================================\n",
      "Epoch 0:\n",
      "GRU Stacked RNN loss: 0.1538355495475933\n",
      "MultilabelAUROC(): 57.15%\n",
      "----------------------------------------\n",
      "Epoch 1:\n",
      "GRU Stacked RNN loss: 0.14035218500200358\n",
      "MultilabelAUROC(): 65.45%\n",
      "----------------------------------------\n",
      "Epoch 2:\n",
      "GRU Stacked RNN loss: 0.13215053417519027\n",
      "MultilabelAUROC(): 69.16%\n",
      "----------------------------------------\n",
      "Epoch 3:\n",
      "GRU Stacked RNN loss: 0.12651434755349497\n",
      "MultilabelAUROC(): 71.66%\n",
      "----------------------------------------\n",
      "Epoch 4:\n",
      "GRU Stacked RNN loss: 0.12183887176996473\n",
      "MultilabelAUROC(): 73.63%\n",
      "----------------------------------------\n",
      "Epoch 5:\n",
      "GRU Stacked RNN loss: 0.11800104988193577\n",
      "MultilabelAUROC(): 74.70%\n",
      "----------------------------------------\n",
      "Epoch 6:\n",
      "GRU Stacked RNN loss: 0.1147981309826266\n",
      "MultilabelAUROC(): 75.96%\n",
      "----------------------------------------\n",
      "Epoch 7:\n",
      "GRU Stacked RNN loss: 0.11193863622355574\n",
      "MultilabelAUROC(): 76.82%\n",
      "----------------------------------------\n",
      "Epoch 8:\n",
      "GRU Stacked RNN loss: 0.10945466937065286\n",
      "MultilabelAUROC(): 77.59%\n",
      "----------------------------------------\n",
      "Epoch 9:\n",
      "GRU Stacked RNN loss: 0.10722974674717005\n",
      "MultilabelAUROC(): 77.96%\n",
      "----------------------------------------\n",
      "========================================\n",
      "Epoch 0:\n",
      "GRU Bidirectional RNN loss: 0.14996631111020248\n",
      "MultilabelAUROC(): 63.67%\n",
      "----------------------------------------\n",
      "Epoch 1:\n",
      "GRU Bidirectional RNN loss: 0.13379379434409056\n",
      "MultilabelAUROC(): 69.60%\n",
      "----------------------------------------\n",
      "Epoch 2:\n",
      "GRU Bidirectional RNN loss: 0.12526047352683795\n",
      "MultilabelAUROC(): 72.90%\n",
      "----------------------------------------\n",
      "Epoch 3:\n",
      "GRU Bidirectional RNN loss: 0.11949234083546748\n",
      "MultilabelAUROC(): 74.70%\n",
      "----------------------------------------\n",
      "Epoch 4:\n",
      "GRU Bidirectional RNN loss: 0.11506396366826097\n",
      "MultilabelAUROC(): 76.17%\n",
      "----------------------------------------\n",
      "Epoch 5:\n",
      "GRU Bidirectional RNN loss: 0.11127139849587016\n",
      "MultilabelAUROC(): 77.31%\n",
      "----------------------------------------\n",
      "Epoch 6:\n",
      "GRU Bidirectional RNN loss: 0.10803485042626508\n",
      "MultilabelAUROC(): 78.12%\n",
      "----------------------------------------\n",
      "Epoch 7:\n",
      "GRU Bidirectional RNN loss: 0.10517553548864451\n",
      "MultilabelAUROC(): 78.85%\n",
      "----------------------------------------\n",
      "Epoch 8:\n",
      "GRU Bidirectional RNN loss: 0.10275898964482437\n",
      "MultilabelAUROC(): 79.50%\n",
      "----------------------------------------\n",
      "Epoch 9:\n",
      "GRU Bidirectional RNN loss: 0.10042842532469663\n",
      "MultilabelAUROC(): 79.97%\n",
      "----------------------------------------\n",
      "========================================\n",
      "Epoch 0:\n",
      "GRU Stacked Bidirectional RNN loss: 0.14731935601608426\n",
      "MultilabelAUROC(): 64.39%\n",
      "----------------------------------------\n",
      "Epoch 1:\n",
      "GRU Stacked Bidirectional RNN loss: 0.13197645728373705\n",
      "MultilabelAUROC(): 70.26%\n",
      "----------------------------------------\n",
      "Epoch 2:\n",
      "GRU Stacked Bidirectional RNN loss: 0.1234213577040633\n",
      "MultilabelAUROC(): 73.81%\n",
      "----------------------------------------\n",
      "Epoch 3:\n",
      "GRU Stacked Bidirectional RNN loss: 0.11711016926030358\n",
      "MultilabelAUROC(): 75.82%\n",
      "----------------------------------------\n",
      "Epoch 4:\n",
      "GRU Stacked Bidirectional RNN loss: 0.11205803888564661\n",
      "MultilabelAUROC(): 77.50%\n",
      "----------------------------------------\n",
      "Epoch 5:\n",
      "GRU Stacked Bidirectional RNN loss: 0.10773942929373309\n",
      "MultilabelAUROC(): 78.65%\n",
      "----------------------------------------\n",
      "Epoch 6:\n",
      "GRU Stacked Bidirectional RNN loss: 0.10389129412968147\n",
      "MultilabelAUROC(): 79.51%\n",
      "----------------------------------------\n",
      "Epoch 7:\n",
      "GRU Stacked Bidirectional RNN loss: 0.10065451299644468\n",
      "MultilabelAUROC(): 80.32%\n",
      "----------------------------------------\n",
      "Epoch 8:\n",
      "GRU Stacked Bidirectional RNN loss: 0.09767621320856834\n",
      "MultilabelAUROC(): 81.07%\n",
      "----------------------------------------\n",
      "Epoch 9:\n",
      "GRU Stacked Bidirectional RNN loss: 0.0950654784546179\n",
      "MultilabelAUROC(): 81.63%\n",
      "----------------------------------------\n",
      "========================================\n",
      "CPU times: user 53min 20s, sys: 2.73 s, total: 53min 23s\n",
      "Wall time: 53min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#var\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = BATCH_SIZE\n",
    "\n",
    "for model in models:\n",
    "    model.to('cuda')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "    \n",
    "    for i in range(EPOCHS):\n",
    "        \n",
    "        train_loss = train(model, loss_fn, optimizer, train_dataloader_var, BATCH_SIZE)\n",
    "        metric = MultilabelAUROC(num_labels=len(emotions), average='macro').to('cuda')\n",
    "        train_metric = test(model, loss_fn, train_dataloader_var, metric, 0.5)\n",
    "        scheduler.step(train_loss)\n",
    "        print(f'Epoch {i}:')\n",
    "        print(f'{model.name} loss: {train_loss}')\n",
    "        print(f'{metric}: {train_metric * 100 :.2f}%')\n",
    "        print('-'*40)\n",
    "    print('='*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3493fca9-bd27-4ae1-845e-13a621cc06f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    model.compile(\n",
    "        loss=torch.nn.BCEWithLogitsLoss(),\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            ...\n",
    "        ),\n",
    "        metrics=[\n",
    "            ...\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d844e36e-34a4-4a42-8818-96d88ad9ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_dataset, test_dataset in datasets:\n",
    "    for model in models:\n",
    "        model.fit(train_dataset, validation_data=test_dataset, epochs=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d19d6cb-a4b7-415e-9dc6-da3623c5a295",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d9afd3-afb6-4549-9a9f-4eb6102694e1",
   "metadata": {},
   "source": [
    "Evaluate the models you trained on the test datasets. Plot ROC curves for each label (use `sklearn.metrics.RocCurveDisplay`) for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f331fc5c-5d19-4fa8-a5e1-3216d9d4457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    model: keras.Model,\n",
    "    ax: plt.Axes | None = None\n",
    ") -> float:\n",
    "    '''Plots ROC curves for each of the labels (on a single axes) and outputs mean ROC AUC score.\n",
    "\n",
    "    Arguments:\n",
    "        X: model inputs\n",
    "        y: ground thruths\n",
    "        model: model to plot the curve for\n",
    "        ax: axes to plot on\n",
    "\n",
    "    Returns:\n",
    "        Mean ROC AUC score'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38c983-510f-47f5-8a11-5344b53c15e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "224c37b8-218a-43d8-b056-2389a3869e8c",
   "metadata": {},
   "source": [
    "Plot the mean ROC AUC scores. Which model has the highest score? On what kind of dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba2b2c8-66ec-40e9-aaec-07ac79c86ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f56b9bca-296b-45cd-be57-6027371d0c5a",
   "metadata": {},
   "source": [
    "Inspect the best model performance closer. Come up with some sentences (in English). Does the model output sensible results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f66686-4319-4762-82d0-963edbad5fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_text(text: str, model: keras.Model, threshold: float = 0.5, max_length: int | None = None) -> list[str]:\n",
    "    '''Computes the model output for `text` and outputs a list of emotions that have a probability of at least `threshold`\n",
    "\n",
    "    Arguments:\n",
    "        text: text to label\n",
    "        model: model to use\n",
    "        threshold: threshold to use\n",
    "        max_length: max length for tokenization\n",
    "    \n",
    "    Return:\n",
    "        List of predicted emotion labels'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59fe95-b472-4b7b-95f9-23d17bba18db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_emotion_scores(text: str, model: keras.Model, max_length: int | None = None, ax: plt.Axes | None = None):\n",
    "    '''Plots a bar plot of emotion probabilities for given `text` using `model`.\n",
    "\n",
    "    Arguments:\n",
    "        text: text to label\n",
    "        model: model to use        \n",
    "        max_length: max length for tokenization\n",
    "        ax: axes to plot on'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e7a3fd-d086-47da-ab93-4927079cd037",
   "metadata": {},
   "source": [
    "For each of your texts get a list of emotion labels and plot emotion scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ae81dd-ab9c-46a0-825f-ab53b367b75c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c98b8e7f-e826-497c-943a-fe944a4f4ba9",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa03eefa-90f1-4be9-9ab0-b17e85993aa1",
   "metadata": {},
   "source": [
    "Train and evaluate the same model as your best one, but use a different cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1a9da-b0a9-4a4d-9d91-b7061fc903b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
